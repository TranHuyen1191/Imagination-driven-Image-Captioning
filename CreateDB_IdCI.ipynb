{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8647ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Code adapted from the data preprocessing code of the ArtEmis paper.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Combine, clean, pre-process ArtEmis annotations.\n",
    "The MIT License (MIT)\n",
    "Originally created by Panos Achlioptas at 6/17/20, for Python 3.x\n",
    "Copyright (c) 2021 Panos Achlioptas (pachlioptas@gmail.com) & Stanford Geometric Computing Lab\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "import argparse\n",
    "import pprint\n",
    "import pathlib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "import pdb\n",
    "random_seed = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fa2685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454684\n"
     ]
    }
   ],
   "source": [
    "## load source dataset\n",
    "source_data_csv = '../Dataset/ArtEmis/OriginalArtEmis/artemis_dataset_full.csv'\n",
    "df = pd.read_csv(source_data_csv)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a29121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images with available input features of M2 model: 80924\n"
     ]
    }
   ],
   "source": [
    "#Load a list of images with input features of M2 model provided by ArtEmis paper\n",
    "with open('../Dataset/ArtEmis/OriginalArtEmis/list_avai_imgId.pkl','rb') as file: \n",
    "    avai_imgID = pickle.load(file)\n",
    "with open('../Dataset/ArtEmis/OriginalArtEmis/wikiart_split.pkl','rb') as file:\n",
    "    paints_ids_dict = dict(pickle.load(file))\n",
    "paints_ids_dict_ids = list(paints_ids_dict.values())\n",
    "paints_ids_dict_imgfiles = list(paints_ids_dict.keys())\n",
    "avai_imgfiles = []\n",
    "for imgid in avai_imgID:\n",
    "    avai_imgfiles.append(paints_ids_dict_imgfiles[imgid])\n",
    "print(f\"Number of images with available input features of M2 model: {len(avai_imgfiles)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3442f472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymSpell spell-checker loaded: True\n",
      "Loading glove word embeddings.\n",
      "Done. 400000 words loaded.\n",
      "Updating Glove vocabulary with *valid* ArtEmis words that are missing from it.\n",
      "tokens not in Glove/Manual vocabulary: 662\n",
      "454684\n"
     ]
    }
   ],
   "source": [
    "## Create utterance_spelled\n",
    "from artemis.language.basics import tokenize_and_spell\n",
    "glove_file = '../Dataset/ArtEmis/OriginalArtEmis/glove.6B.100d.vocabulary.txt'\n",
    "freq_file = '../Dataset/ArtEmis/OriginalArtEmis/symspell_frequency_dictionary_en_82_765.txt'\n",
    "missed_tokens = tokenize_and_spell(df, glove_file, freq_file, nltk.word_tokenize, spell_check=True)\n",
    "print('tokens not in Glove/Manual vocabulary:', len(missed_tokens))\n",
    "print(len(df.utterance_spelled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181c5427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453525\n"
     ]
    }
   ],
   "source": [
    "too_long_cap = df.tokens_len > 63\n",
    "df = df[~too_long_cap]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7531a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400838\n"
     ]
    }
   ],
   "source": [
    "## Exclude captions with emotions = 'something else'\n",
    "df = df[df.emotion!='something else']\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f01b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract imagination-driven captions\n",
    "# List of keywords\n",
    "keywords_2tokens = {'looks like','look like','look as','looks as','reminds me','remind me',\n",
    "                       'is like','is likely','are like','are likely','think of','thinks of',\n",
    "                       'as if','as though','feel like','feels like','shaped like', 'shapes like', 'shape like',\n",
    "                       'calm like','looks likely','look likely',\n",
    "                       'seems like','seem like','seems as', 'seem as',\n",
    "                    }\n",
    "\n",
    "keywords_3tokens = {'looks almost like','look almost like','is almost as','are almost as','seems to be', 'seem to be'}\n",
    "keywords_1tokens = {'resemble','resembling'}\n",
    "keywords_1tokens_spell = []\n",
    "keywords_2tokens_spell = []\n",
    "keywords_3tokens_spell = []\n",
    "for keyword in keywords_1tokens:\n",
    "    keywords_1tokens_spell.append(((keyword.split(' '))))\n",
    "for keyword in keywords_2tokens:\n",
    "    keywords_2tokens_spell.append(((keyword.split(' '))))\n",
    "for keyword in keywords_3tokens:\n",
    "    keywords_3tokens_spell.append(((keyword.split(' '))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bcaef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "subjects = []\n",
    "subjects_maxLen = 0\n",
    "predicates = []\n",
    "predicates_maxLen = 0\n",
    "for index,tokens_encoded in enumerate(df['tokens']):\n",
    "    subject = None\n",
    "    predicate = None\n",
    "    for i,currToken in enumerate(tokens_encoded):\n",
    "        currToken = tokens_encoded[i:i+1]\n",
    "        if currToken in keywords_1tokens_spell:\n",
    "            if len(tokens_encoded[i:])>=2 and len(tokens_encoded[:i])>=1:\n",
    "                subject = tokens_encoded[:i]\n",
    "                predicate = tokens_encoded[i:]\n",
    "                if subjects_maxLen < len(subject):\n",
    "                    subjects_maxLen = len(subject)\n",
    "                if predicates_maxLen < len(predicate):\n",
    "                    predicates_maxLen = len(predicate)\n",
    "                break # Stop at the first keyword in the sentence\n",
    "            elif len(tokens_encoded[:i]) ==0:\n",
    "                subject = ['it']\n",
    "                predicate = tokens_encoded[i:]\n",
    "                if subjects_maxLen < len(subject):\n",
    "                    subjects_maxLen = len(subject)\n",
    "                if predicates_maxLen < len(predicate):\n",
    "                    predicates_maxLen = len(predicate)\n",
    "                break # Stop at the first keyword in the sentence\n",
    "        if i >= 1:\n",
    "            contToken = tokens_encoded[i-1:i+1]\n",
    "            if contToken in keywords_2tokens_spell:\n",
    "                if len(tokens_encoded[i:])>=2  and len(tokens_encoded[:i-1])>=1:\n",
    "                    subject = tokens_encoded[:i-1]\n",
    "                    predicate = tokens_encoded[i-1:]\n",
    "                    if subjects_maxLen < len(subject):\n",
    "                        subjects_maxLen = len(subject)\n",
    "                    if predicates_maxLen < len(predicate):\n",
    "                        predicates_maxLen = len(predicate)\n",
    "                    break # Stop at the first keyword in the sentence\n",
    "                elif len(tokens_encoded[:i-1]) ==0:\n",
    "                    subject = ['it']\n",
    "                    predicate = tokens_encoded[i-1:]\n",
    "                    if subjects_maxLen < len(subject):\n",
    "                        subjects_maxLen = len(subject)\n",
    "                    if predicates_maxLen < len(predicate):\n",
    "                        predicates_maxLen = len(predicate)\n",
    "                    break # Stop at the first keyword in the sentence\n",
    "        if i >= 2:\n",
    "            contToken = tokens_encoded[i-2:i+1]\n",
    "            if contToken in keywords_3tokens_spell:\n",
    "                if len(tokens_encoded[i:])>=2   and len(tokens_encoded[:i-2])>=1:\n",
    "                    subject = tokens_encoded[:i-2]\n",
    "                    predicate = tokens_encoded[i-2:]\n",
    "                    if subjects_maxLen < len(subject):\n",
    "                        subjects_maxLen = len(subject)\n",
    "                    if predicates_maxLen < len(predicate):\n",
    "                        predicates_maxLen = len(predicate)\n",
    "                    break # Stop at the first keyword in the sentence\n",
    "                elif len(tokens_encoded[:i-2]) ==0:\n",
    "                    subject = ['it']\n",
    "                    predicate = tokens_encoded[i-2:]\n",
    "                    if subjects_maxLen < len(subject):\n",
    "                        subjects_maxLen = len(subject)\n",
    "                    if predicates_maxLen < len(predicate):\n",
    "                        predicates_maxLen = len(predicate)\n",
    "                    break # Stop at the first keyword in the sentence\n",
    "    subjects.append(subject)\n",
    "    predicates.append(predicate)\n",
    "print(subjects_maxLen)\n",
    "print(predicates_maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d504d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subject']=subjects\n",
    "df['predicate']=predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45cff8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IdCI = df[[subject != None for subject in df.subject]].copy()\n",
    "df_IdCI.reset_index(drop=True, inplace=True)\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f33bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete images whose input features of M2 model are not provided\n",
    "img_files = '/'+df_IdCI.art_style+'/'+df_IdCI.painting\n",
    "sel_img_idx = []\n",
    "for img_file in img_files.tolist():\n",
    "    sel_img_idx.append(img_file in avai_imgfiles)\n",
    "df_IdCI = df_IdCI[sel_img_idx]\n",
    "df_IdCI.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "530b7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the repetition of each artwork based on unique_id\n",
    "df_IdCI['unique_id'] = df_IdCI.art_style+ df_IdCI.painting\n",
    "df_IdCI['repetition'] =  df_IdCI.groupby('unique_id')['unique_id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a667ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract emotion distributions\n",
    "ARTEMIS_EMOTIONS = ['amusement', 'awe', 'contentment', 'excitement',\n",
    "                    'anger', 'disgust',  'fear', 'sadness']\n",
    "\n",
    "EMOTION_TO_IDX = {e: i for i, e in enumerate(ARTEMIS_EMOTIONS)}\n",
    "no_emo = len(ARTEMIS_EMOTIONS)\n",
    "no_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d8bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TO_EMOTION = {EMOTION_TO_IDX[e]: e for e in EMOTION_TO_IDX}\n",
    "df_IdCI['emotion_label'] = df_IdCI.emotion.apply(lambda emotion: EMOTION_TO_IDX[emotion])\n",
    "def cal_hist(x):\n",
    "    no_caps = len(x)\n",
    "    dis = [list(x).count(i) for i in range(no_emo)]    \n",
    "    dis = np.array(dis)/no_caps \n",
    "    return list(list(list([list(dis),]*no_caps)))\n",
    "\n",
    "df_IdCI['distEmo'] = df_IdCI.groupby('unique_id')['emotion_label'].transform(cal_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6bc5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75509\n",
      "9000\n",
      "15884\n"
     ]
    }
   ],
   "source": [
    "## Split dataset\n",
    "val_size =  3000\n",
    "## Splits to train, val, test sets\n",
    "train = [unique_id for unique_id,repetition in zip(df_IdCI.unique_id,df_IdCI.repetition)  if repetition <=2 ]\n",
    "test = [unique_id for unique_id,repetition in zip(df_IdCI.unique_id,df_IdCI.repetition)  if repetition >=4 ]\n",
    "rest = [unique_id for unique_id,repetition in zip(df_IdCI.unique_id,df_IdCI.repetition)  if repetition >2 and repetition <4 ]\n",
    "val  = []\n",
    "\n",
    "#Get unique values\n",
    "train = list(set(train))\n",
    "test = list(set(test))\n",
    "rest = list(set(rest))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "rest.sort()\n",
    "train_2, val = train_test_split(rest, test_size=val_size, random_state=random_seed)\n",
    "train = train + train_2\n",
    "\n",
    "train = set(train)\n",
    "test = set(test)\n",
    "val = set(val)\n",
    "assert len(test.intersection(train)) == 0\n",
    "assert len(val.intersection(train)) == 0\n",
    "assert len(test.intersection(val)) == 0\n",
    "\n",
    "df_IdCI['split'] =  ['train' if uni_id in train  else 'val' if uni_id in val  else 'test' for uni_id in df_IdCI.unique_id ]\n",
    "print(len(df_IdCI[df_IdCI.split == 'train']) )\n",
    "print(len(df_IdCI[df_IdCI.split == 'val']) )\n",
    "print(len(df_IdCI[df_IdCI.split == 'test']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b213a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary with 10506 tokens\n"
     ]
    }
   ],
   "source": [
    "# Make a word-vocabulary based on training data\n",
    "from artemis.utils.vocabulary import build_vocab\n",
    "min_word_freq = 3\n",
    "train_tokens = df_IdCI[df_IdCI.split =='train']['tokens']\n",
    "vocab = build_vocab(train_tokens, min_word_freq)\n",
    "print(f'Using a vocabulary with {len(vocab)} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6ed51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode tokens as ints\n",
    "max_len = max(df_IdCI.tokens_len)\n",
    "df_IdCI['tokens_encoded'] = df_IdCI.tokens.apply(lambda x: vocab.encode(x, max_len))\n",
    "df_IdCI['subject_encoded'] = df_IdCI.subject.apply(lambda x: vocab.encode(x, subjects_maxLen))\n",
    "df_IdCI['predicate_encoded'] = df_IdCI.predicate.apply(lambda x: vocab.encode(x, predicates_maxLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0814e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode tokens using CLIP tokenizer\n",
    "import clip \n",
    "df_IdCI['CLIP_tokens'] = [clip.tokenize(utter).squeeze().tolist() for utter in df_IdCI['utterance_spelled']]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d223000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save separately the grouped utterances of each stimulus\n",
    "def group_gt_annotations(df, vocab):\n",
    "    \"\"\" Group the annotations according to the underlying artwork/stimulus.\n",
    "    :param preprocessed_dataframe: dataframe carrying ArtEmis annotations, spell-checked, with splits etc.\n",
    "    :param vocab: the corresponding Vocabulary object\n",
    "    :return: dictionary, carrying for each split (tran/test/val) a dataframe that has for each artwork all its collected\n",
    "        annotations grouped.\n",
    "    \"\"\"\n",
    "    results = dict()\n",
    "    for split, g in df.groupby('split'): # group-by split\n",
    "        g.reset_index(inplace=True, drop=True)\n",
    "        g = g.groupby(['art_style', 'painting']) # group-by stimulus\n",
    "\n",
    "        # group utterances / emotions\n",
    "        # a) before \"vocabularization\" (i.e., raw)\n",
    "        refs_pre_vocab_grouped = g['utterance_spelled'].apply(list).reset_index(name='references_pre_vocab')\n",
    "        # np.sum(refs_pre_vocab_grouped.duplicated(subset=['painting']))\n",
    "        # b) post \"vocabularization\" (e.g., contain <UNK>)\n",
    "        #print(len(refs_pre_vocab_grouped.iloc[2]['references_pre_vocab']))\n",
    "\n",
    "        tokens_grouped = g['tokens_encoded'].apply(list).reset_index(name='tokens_encoded')\n",
    "        #print(len(tokens_grouped.iloc[2]['tokens_encoded']))\n",
    "        emotion_grouped = g['emotion_label'].apply(list).reset_index(name='emotion')\n",
    "        #print(len(emotion_grouped.iloc[2]['emotion']))\n",
    "\n",
    "        assert all(tokens_grouped['painting'] == emotion_grouped['painting'])\n",
    "        assert all(tokens_grouped['painting'] == refs_pre_vocab_grouped['painting'])\n",
    "\n",
    "        # decode these tokens back to strings and name them \"references\"\n",
    "        tokens_grouped['tokens_encoded'] =\\\n",
    "            tokens_grouped['tokens_encoded'].apply(lambda x: [vocab.decode_print(sent) for sent in x])\n",
    "        tokens_grouped = tokens_grouped.rename(columns={'tokens_encoded': 'references'})\n",
    "\n",
    "        # join results in a new single dataframe\n",
    "        temp = pd.merge(emotion_grouped, refs_pre_vocab_grouped)\n",
    "        #print(len(temp.iloc[2]['emotion']))\n",
    "        result = pd.merge(temp, tokens_grouped)\n",
    "        #print(len(result.iloc[2]['references']))\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        results[split] = result\n",
    "    return results\n",
    "\n",
    "groups = group_gt_annotations(df_IdCI, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3442276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-utterances kept: 100393\n",
      "vocab size: 10506\n",
      "Maximum number of tokens per caption is 63\n",
      "Minimum number of tokens per caption is 3\n"
     ]
    }
   ],
   "source": [
    "from six.moves import cPickle\n",
    "def pickle_data(file_name, *args):\n",
    "    \"\"\"Using (c)Pickle to save multiple python objects in a single file.\n",
    "    \"\"\"\n",
    "    out_file = open(file_name, 'wb')\n",
    "    cPickle.dump(len(args), out_file, protocol=2)\n",
    "    for item in args:\n",
    "        cPickle.dump(item, out_file, protocol=2)\n",
    "    out_file.close()\n",
    "    \n",
    "\n",
    "df_IdCI.reset_index(drop=True,inplace=True)\n",
    "df_IdCI.to_csv(f'../Dataset/ArtEmis/ArtEmis_IdC/ArtEmis_IdCI.csv', index=False)\n",
    "vocab.save(f'../Dataset/ArtEmis/ArtEmis_IdC/ArtEmis_IdCI_Vocab.pkl')\n",
    "pickle_data(f'../Dataset/ArtEmis/ArtEmis_IdC/Artemis_IdCI_GT.pkl', groups)\n",
    "\n",
    "print('n-utterances kept:', len(df_IdCI))\n",
    "print('vocab size:', len(vocab))\n",
    "print(f'Maximum number of tokens per caption is {max_len}')\n",
    "print(f'Minimum number of tokens per caption is {min(df_IdCI.tokens_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d60604e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  captions per image:  30155  images with 30155  captions\n",
      "2  captions per image:  17811  images with 35622  captions\n",
      "3  captions per image:  6244  images with 18732  captions\n",
      ">=4 captions per image:  2497  images with 15884  captions\n",
      "Total images: 56707\n",
      "Total captions: 100393\n"
     ]
    }
   ],
   "source": [
    "### Extract number of images having the number of captions = noCap\n",
    "for noCap in range(1,4):\n",
    "    cnt = 0\n",
    "    cntexp = 0\n",
    "    for name, group in df_IdCI.groupby('unique_id'):\n",
    "        #print(group)\n",
    "        #break\n",
    "        if group.repetition.iloc[0] ==noCap:\n",
    "            #print(group.freq)\n",
    "            cnt= cnt + 1\n",
    "            cntexp += len(group)\n",
    "    print(noCap,\" captions per image: \",cnt,\" images with\",cntexp,\" captions\")\n",
    "cnt = 0\n",
    "cntexp = 0\n",
    "for name, group in df_IdCI.groupby('unique_id'):\n",
    "    if group.repetition.iloc[0] >=4:\n",
    "        #print(group.freq)\n",
    "        cnt= cnt + 1\n",
    "        cntexp += len(group)\n",
    "print(\">=4 captions per image: \",cnt,\" images with\",cntexp,\" captions\")\n",
    "print('Total images:',len(df_IdCI.unique_id.unique()))\n",
    "print('Total captions:',len(df_IdCI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82baa049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
