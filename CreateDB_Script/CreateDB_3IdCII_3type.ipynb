{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import clip\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the  dataset (merge it with the emotion-histograms.)\n",
    "datasetname = 'ArtEmis' #ArtEmis, Flickr30K,  VizWiz, COCO\n",
    "\n",
    "if datasetname == 'ArtEmis':\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdC/{datasetname}_IdCII_TypeII_addText.csv'\n",
    "    outfile = f'../Dataset/{datasetname}/{datasetname}_IdC/{datasetname}_IdCII_3ErrType.csv'\n",
    "    df_full = pd.read_csv(datafile)\n",
    "    df_full['img_files'] = [osp.join(art_style,painting+'.jpg') for (art_style,painting) in zip(df_full.art_style.tolist(),df_full.painting.tolist())]\n",
    "else:\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdCII_TypeII_addText.csv'\n",
    "    outfile = f'../Dataset/{datasetname}/{datasetname}_IdCII_3ErrType.csv'\n",
    "    df_full = pd.read_csv(datafile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations loaded: 100393\n"
     ]
    }
   ],
   "source": [
    "print('Annotations loaded:', len(df_full))\n",
    "df_full['captSet_CLIP_tokens'] = df_full['captSet_CLIP_tokens'].apply(literal_eval)\n",
    "df_full['captSet_text'] = df_full['captSet_text'].apply(literal_eval)\n",
    "df_full['subject'] = df_full['subject'].apply(literal_eval)\n",
    "df_full['predicate'] = df_full['predicate'].apply(literal_eval)\n",
    "df_full['CLIP_tokens'] = df_full['CLIP_tokens'].apply(literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Only using one generated caption for each error type \n",
    "# captSet: Natural caption --> Type I --> Type II --> Type III\n",
    "# refCaptSet: Other captions\n",
    "df_full['captSet_CLIP_tokens'] = df_full['captSet_CLIP_tokens'].apply(lambda x: x[0:2])\n",
    "df_full['captSet_text'] = df_full['captSet_text'].apply(lambda x: x[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create unnatural captions of type I and III for the training set\n",
    "import nltk\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "df_new = pd.DataFrame(columns = df_full.columns)\n",
    "cnt = 0\n",
    "for imgfile,g in df_full.groupby(['img_files']):\n",
    "    other_imgs_df = df_full[df_full.img_files != imgfile]\n",
    "    other_imgs_df.reset_index(inplace=True, drop=True)\n",
    "    gtCapts = g.utterance_spelled.tolist()\n",
    "    gtCapts_CLIP_tokens = g.CLIP_tokens.tolist()\n",
    "    #print(gtCapts)\n",
    "    #print(gtCapts_CLIP_tokens)\n",
    "    for index, row in g.iterrows():\n",
    "        ## Create refCaptSet by excluding the current caption\n",
    "        refCaptSet = gtCapts.copy()\n",
    "        refCaptSet.remove(row['utterance_spelled'])\n",
    "        refCaptSet_CLIP_tokens = gtCapts_CLIP_tokens.copy()\n",
    "        refCaptSet_CLIP_tokens.remove(row['CLIP_tokens'])\n",
    "        #print(refCaptSet)\n",
    "        #print(refCaptSet_CLIP_tokens)\n",
    "        row['refCaptSet'] = refCaptSet\n",
    "        row['refCaptSet_CLIP_tokens'] = refCaptSet_CLIP_tokens\n",
    "        \n",
    "        ## Add natural captions\n",
    "        captSet = row['captSet_text'][0:1]\n",
    "        captSet_CLIP_tokens = row['captSet_CLIP_tokens'][0:1]\n",
    "        \n",
    "        #Add Error type I\n",
    "        while True: #only using subject with len > 3 words --> avoid subject = 'this'\n",
    "            sel_row = other_imgs_df.iloc[random.randint(0,len(other_imgs_df)-1)]\n",
    "            if len(sel_row['subject'])>3:\n",
    "                unCapt = sel_row['subject'] + row['predicate']\n",
    "                if len(unCapt) > 65:## too long for CLIP tokenizer\n",
    "                    unCapt = unCapt[:65]\n",
    "                unCapt = ' '.join(unCapt)\n",
    "                unCapt_CLIP_tokens = clip.tokenize(unCapt).squeeze().tolist()\n",
    "                captSet.append(unCapt)\n",
    "                captSet_CLIP_tokens.append(unCapt_CLIP_tokens)\n",
    "                break\n",
    "        \n",
    "        ## Add Error Type II\n",
    "        captSet.append(row['captSet_text'][1])\n",
    "        captSet_CLIP_tokens.append(row['captSet_CLIP_tokens'][1])\n",
    "        \n",
    "        #Add Error type III Incompletion\n",
    "        words = row['utterance_spelled'].split(' ')\n",
    "        #print(len(row['predicate']))\n",
    "        no_remove_word = random.randint(1,max(1,int(len(row['predicate'])/2)))\n",
    "        #print(\"no_remove_word\",no_remove_word)\n",
    "        #print(words)\n",
    "        len_temp = len(words)\n",
    "        words = words[:-no_remove_word]\n",
    "        assert len_temp> len(words)\n",
    "        #print(words)\n",
    "        unCapt = ' '.join(words)\n",
    "        #print(unCapt)\n",
    "        unCapt_CLIP_tokens = clip.tokenize(unCapt).squeeze().tolist()\n",
    "        #print(unCapt_CLIP_tokens)\n",
    "        captSet.append(unCapt)\n",
    "        assert len(captSet) == 4\n",
    "        captSet_CLIP_tokens.append(unCapt_CLIP_tokens)\n",
    "        row['captSet_text'] = captSet\n",
    "        row['captSet_CLIP_tokens'] = captSet_CLIP_tokens\n",
    "        df_new = df_new.append(row)\n",
    "        cnt +=1\n",
    "    #if cnt >3:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.reset_index(drop=True,inplace=True)\n",
    "df_new.to_csv( outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100393\n"
     ]
    }
   ],
   "source": [
    "print(len(df_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the patterns and assorted colors look like a time in chinese history well mix of colors',\n",
       " 'the man has lost the majority of his hair so he look like a time in chinese history well mix of colors',\n",
       " 'the patterns and assorted colors resemble a clown',\n",
       " 'the patterns and assorted colors look like a time in chinese history well mix']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch13",
   "language": "python",
   "name": "pytorch13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
