{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3eb1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Code adapted from the source code of ArtEmis dataset paper\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from model.func_eval import unpickle_data,torch_load_model\n",
    "from artemis.utils.vocabulary import Vocabulary #Use for text2emotion metrics\n",
    "from artemis.evaluation.single_caption_per_image import apply_basic_evaluations\n",
    "from artemis.emotions import IDX_TO_EMOTION\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8475c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBdir = \"../Dataset/ArtEmis/ArtEmis_IdC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5520a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_file = osp.join(DBdir,'Artemis_IdCI_GT.pkl')\n",
    "split = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57021843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Captioned 2497\n"
     ]
    }
   ],
   "source": [
    "gt_data = next(unpickle_data(references_file))\n",
    "gt_data = gt_data[split]\n",
    "print('Images Captioned', len(gt_data))\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15aecd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = f'../Dataset/ArtEmis/ArtEmis_IdC/Images/rawImages/'\n",
    "gt_data['img_file'] = [osp.join(img_dir, row.art_style,  row.painting + '.jpg') for _,row in gt_data.iterrows()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820ee544",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract features using code of https://github.com/facebookresearch/vilbert-multi-task/tree/main/script/extract_features.py\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.layers import nms\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
    "import os\n",
    "import cv2\n",
    "class FeatureExtractor:\n",
    "    MAX_SIZE = 1333\n",
    "    MIN_SIZE = 800\n",
    "\n",
    "    def __init__(self,model_file,config_file,batch_size=2,num_features=100,output_folder=\"./output\",\n",
    "                 feature_name=\"fc6\",confidence_threshold=0,background=False,partition=0):\n",
    "        self.model_file = model_file\n",
    "        self.config_file = config_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_features = num_features\n",
    "        self.output_folder = output_folder\n",
    "        self.feature_name = feature_name\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.background = background\n",
    "        self.partition = partition\n",
    "        \n",
    "        self.detection_model = self._build_detection_model()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "\n",
    "    def _build_detection_model(self):\n",
    "        cfg.merge_from_file(self.config_file)\n",
    "        cfg.freeze()\n",
    "\n",
    "        model = build_detection_model(cfg)\n",
    "        checkpoint = torch.load(self.model_file, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "        load_state_dict(model, checkpoint.pop(\"model\"))\n",
    "\n",
    "        model.to(\"cuda\")\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def _image_transform(self, path):\n",
    "        img = Image.open(path)\n",
    "        im = np.array(img).astype(np.float32)\n",
    "        # IndexError: too many indices for array, grayscale images\n",
    "        if len(im.shape) < 3:\n",
    "            im = np.repeat(im[:, :, np.newaxis], 3, axis=2)\n",
    "        im = im[:, :, ::-1]\n",
    "        im -= np.array([102.9801, 115.9465, 122.7717])\n",
    "        im_shape = im.shape\n",
    "        im_height = im_shape[0]\n",
    "        im_width = im_shape[1]\n",
    "        im_size_min = np.min(im_shape[0:2])\n",
    "        im_size_max = np.max(im_shape[0:2])\n",
    "\n",
    "        # Scale based on minimum size\n",
    "        im_scale = self.MIN_SIZE / im_size_min\n",
    "\n",
    "        # Prevent the biggest axis from being more than max_size\n",
    "        # If bigger, scale it down\n",
    "        if np.round(im_scale * im_size_max) > self.MAX_SIZE:\n",
    "            im_scale = self.MAX_SIZE / im_size_max\n",
    "\n",
    "        im = cv2.resize(\n",
    "            im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "        img = torch.from_numpy(im).permute(2, 0, 1)\n",
    "\n",
    "        im_info = {\"width\": im_width, \"height\": im_height}\n",
    "\n",
    "        return img, im_scale, im_info\n",
    "\n",
    "    def _process_feature_extraction(\n",
    "        self, output, im_scales, im_infos, feature_name=\"fc6\", conf_thresh=0\n",
    "    ):\n",
    "        batch_size = len(output[0][\"proposals\"])\n",
    "        n_boxes_per_image = [len(boxes) for boxes in output[0][\"proposals\"]]\n",
    "        score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
    "        score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
    "        feats = output[0][feature_name].split(n_boxes_per_image)\n",
    "        cur_device = score_list[0].device\n",
    "\n",
    "        feat_list = []\n",
    "        info_list = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
    "            scores = score_list[i]\n",
    "            max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
    "            conf_thresh_tensor = torch.full_like(max_conf, conf_thresh)\n",
    "            start_index = 1\n",
    "            # Column 0 of the scores matrix is for the background class\n",
    "            if self.background:\n",
    "                start_index = 0\n",
    "            for cls_ind in range(start_index, scores.shape[1]):\n",
    "                cls_scores = scores[:, cls_ind]\n",
    "                keep = nms(dets, cls_scores, 0.5)\n",
    "                max_conf[keep] = torch.where(\n",
    "                    # Better than max one till now and minimally greater than conf_thresh\n",
    "                    (cls_scores[keep] > max_conf[keep])\n",
    "                    & (cls_scores[keep] > conf_thresh_tensor[keep]),\n",
    "                    cls_scores[keep],\n",
    "                    max_conf[keep],\n",
    "                )\n",
    "\n",
    "            sorted_scores, sorted_indices = torch.sort(max_conf, descending=True)\n",
    "            num_boxes = (sorted_scores[: self.num_features] != 0).sum()\n",
    "            keep_boxes = sorted_indices[: self.num_features]\n",
    "            feat_list.append(feats[i][keep_boxes])\n",
    "            bbox = output[0][\"proposals\"][i][keep_boxes].bbox / im_scales[i]\n",
    "            # Predict the class label using the scores\n",
    "            objects = torch.argmax(scores[keep_boxes][start_index:], dim=1)\n",
    "            cls_prob = torch.max(scores[keep_boxes][start_index:], dim=1)\n",
    "\n",
    "            info_list.append(\n",
    "                {\n",
    "                    \"bbox\": bbox.cpu().numpy(),\n",
    "                    \"num_boxes\": num_boxes.item(),\n",
    "                    \"objects\": objects.cpu().numpy(),\n",
    "                    \"image_width\": im_infos[i][\"width\"],\n",
    "                    \"image_height\": im_infos[i][\"height\"],\n",
    "                    \"cls_prob\": scores[keep_boxes].cpu().numpy(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return feat_list, info_list\n",
    "\n",
    "    def get_detectron_features(self, image_paths):\n",
    "        img_tensor, im_scales, im_infos = [], [], []\n",
    "\n",
    "        for image_path in image_paths:\n",
    "            im, im_scale, im_info = self._image_transform(image_path)\n",
    "            img_tensor.append(im)\n",
    "            im_scales.append(im_scale)\n",
    "            im_infos.append(im_info)\n",
    "\n",
    "        # Image dimensions should be divisible by 32, to allow convolutions\n",
    "        # in detector to work\n",
    "        current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "        current_img_list = current_img_list.to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.detection_model(current_img_list)\n",
    "\n",
    "        feat_list = self._process_feature_extraction(\n",
    "            output,\n",
    "            im_scales,\n",
    "            im_infos,\n",
    "            self.feature_name,\n",
    "            self.confidence_threshold,\n",
    "        )\n",
    "\n",
    "        return feat_list\n",
    "\n",
    "    def _chunks(self, array, chunk_size):\n",
    "        for i in range(0, len(array), chunk_size):\n",
    "            yield array[i : i + chunk_size]\n",
    "\n",
    "    def _save_feature(self, file_name, feature, info):\n",
    "        file_base_name = os.path.basename(file_name)\n",
    "        file_type = file_name.split('/')[-2]\n",
    "        file_base_name = file_base_name[:-4]\n",
    "        info[\"image_id\"] = file_base_name\n",
    "        info[\"features\"] = feature.cpu().numpy()\n",
    "        file_base_name = file_base_name + \".npy\"\n",
    "        save_fd = os.path.join(self.output_folder,file_type)\n",
    "        os.makedirs(save_fd, exist_ok=True)\n",
    "        np.save(os.path.join(save_fd, file_base_name), info)\n",
    "\n",
    "    def extract_features(self,files):\n",
    "        for chunk in self._chunks(files, self.batch_size):\n",
    "            try:\n",
    "                features, infos = self.get_detectron_features(chunk)\n",
    "                for idx, file_name in enumerate(chunk):\n",
    "                    self._save_feature(file_name, features[idx], infos[idx])\n",
    "            except BaseException:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b9b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor('ViLBERTScore/data/detectron_model.pth','ViLBERTScore/data/detectron_config.yaml',output_folder='output/feat_ViLBERTScore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02aa6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huyentran/anaconda3/envs/pytorch13/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811805959/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.extract_features(gt_data['img_file'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56487550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
