{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from model.AdapterCLIPMetric_3MLP import adapterCLIP as adapterCLIP_model\n",
    "from model.func_train_v2 import load_state_dicts\n",
    "import clip\n",
    "from artemis.in_out.neural_net_oriented import read_saved_args\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "from PIL import Image\n",
    "from model.argument import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreezeCase = 1\n",
    "runRawImg = True\n",
    "CLIP_name = 'RN50x16' #RN50, RN101, or RN50x4\n",
    "no_ErrorTypes = 3\n",
    "output_dir = f\"output/adapterCLIP_3MLP/{CLIP_name}_F{FreezeCase}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = osp.join(output_dir, 'checkpoints')\n",
    "saved_model_file = osp.join(model_dir,  'best_model.pt')\n",
    "arg_file = osp.join(output_dir, 'config.json.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "origCLIP,CLIPtransform,CLIPsettings = clip.load(CLIP_name,\"cpu\")\n",
    "embed_dim,image_resolution, vision_layers, vision_width, vision_patch_size,context_length_CLIP, vocab_size_CLIP, transformer_width, transformer_heads, transformer_layers = CLIPsettings\n",
    "\n",
    "if runRawImg:\n",
    "    img_transform = Compose([ \n",
    "                        Resize(image_resolution, interpolation=BICUBIC),\n",
    "                        CenterCrop(image_resolution),\n",
    "                        lambda image: image.convert(\"RGB\"),\n",
    "                        ToTensor(),\n",
    "                        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "                    ])\n",
    "else:\n",
    "    raise ValueError(\"Do not support runRawImg != True!\")\n",
    "    #img_transform = Compose([ToTensor()])\n",
    "\n",
    "adapterCLIP = adapterCLIP_model(embed_dim,image_resolution,vision_layers,vision_width,\n",
    "            vision_patch_size,context_length_CLIP,\n",
    "            vocab_size_CLIP,transformer_width,transformer_heads,transformer_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Copy weights of original CLIP \n",
    "def copy_params(model_init,model):\n",
    "    state_dict_init = model_init.state_dict()\n",
    "    model.load_state_dict(state_dict_init,strict=False)\n",
    "    return True\n",
    "\n",
    "copy_params(origCLIP,adapterCLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at epoch= 3\n"
     ]
    }
   ],
   "source": [
    "args = read_saved_args(arg_file)\n",
    "loaded_epoch = load_state_dicts(saved_model_file, map_location='cpu', model=adapterCLIP)\n",
    "print(\"load model at epoch=\",loaded_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caption sets in the test set: 15884\n"
     ]
    }
   ],
   "source": [
    "## Prepare the  dataset (merge it with the emotion-histograms.)\n",
    "datasetname = 'ArtEmis' #ArtEmis, Flickr30K,  VizWiz, COCO\n",
    "assert datasetname == 'ArtEmis' ## Only work with ArtEmis\n",
    "if datasetname == 'ArtEmis':\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdC/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = '../Dataset/ArtEmis/OriginalArtEmis/wikiart/'\n",
    "    df = pd.read_csv(datafile)\n",
    "    df = df[df.split=='test']\n",
    "else:\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print('Number of caption sets in the test set:', len(df))\n",
    "df.img_files = [osp.join(img_dir,imgfile) for imgfile in df.img_files]\n",
    "df['captSet_CLIP_tokens'] = df['captSet_CLIP_tokens'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=384, interpolation=bicubic, max_size=None, antialias=None)\n",
      "    CenterCrop(size=(384, 384))\n",
      "    <function <lambda> at 0x7f3e545fc598>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class IdCIIDataset(Dataset):\n",
    "    def __init__(self, image_files,captsets,img_transform=None):\n",
    "        super().__init__()\n",
    "        self.image_files = image_files\n",
    "        self.captsets = captsets\n",
    "        self.img_transform = img_transform\n",
    "        print(img_transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        captset = np.array(self.captsets[index]).astype(dtype=np.long)\n",
    "        if self.image_files is not None:\n",
    "            img = Image.open(self.image_files[index])\n",
    "\n",
    "            if img.mode is not 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            if self.img_transform is not None:\n",
    "                img = self.img_transform(img)\n",
    "        else:\n",
    "            img = []\n",
    "        item = {'image_file': self.image_files[index],'image': img, 'captset': captset, 'index': index}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "def preprocess_Dataset(df, img_transform):\n",
    "    set_seed(args.random_seed)\n",
    "    datasets = dict()\n",
    "    for split, g in df.groupby('split'):\n",
    "        g.reset_index(inplace=True, drop=True) \n",
    "        img_files = None\n",
    "        img_files = g.img_files\n",
    "        img_files.name = 'image_files'\n",
    "\n",
    "        dataset = IdCIIDataset(img_files, g.captSet_CLIP_tokens,img_transform=img_transform)\n",
    "\n",
    "        datasets[split] = dataset\n",
    "\n",
    "    dataloaders = dict()\n",
    "    for split in datasets:\n",
    "        if split=='train' or split == 'val':\n",
    "            b_size = args.batch_size  \n",
    "        else:\n",
    "            b_size = 1\n",
    "        dataloaders[split] = torch.utils.data.DataLoader(dataset=datasets[split],\n",
    "                                                         batch_size=b_size,\n",
    "                                                         shuffle=split=='train')\n",
    "    return dataloaders, datasets\n",
    "dataloaders, datasets = preprocess_Dataset(df,img_transform)\n",
    "dataset = datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15884\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "ita_scores = []\n",
    "g_scores = []\n",
    "no_imgs =len(dataset)\n",
    "print(no_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "adapterCLIP.to(device)\n",
    "adapterCLIP.eval()\n",
    "for i in range(no_imgs):\n",
    "    print(i)\n",
    "    data = dataset[i]   \n",
    "    image_inputs = torch.unsqueeze(data['image'], 0)\n",
    "    text_inputs  = torch.LongTensor(data['captset']) \n",
    "    # Calculate features\n",
    "    ita1_score_per_image,_,ita2_score_per_image,_,g_score,score = adapterCLIP(image_inputs.to(device),text_inputs.to(device))\n",
    "    ita_score_per_image = (ita1_score_per_image+ita2_score_per_image)/2\n",
    "    #score = (ita_score_per_image+g_score)/2\n",
    "    scores.append(score.squeeze(0).tolist()) #torch.Size([1, 101])\n",
    "    ita_scores.append(ita_score_per_image.squeeze(0).tolist()) #torch.Size([1, 101])\n",
    "    g_scores.append(g_score.squeeze(0).tolist()) #torch.Size([1, 101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1713,  0.0059, -0.0076, -0.1482]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita1_score_per_image - ita2_score_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/adapterCLIP_3MLP/RN50x16_F1/g.npy\n"
     ]
    }
   ],
   "source": [
    "with open(osp.join(output_dir,'ita.npy'), 'wb') as f:\n",
    "    np.save(f, np.array(ita_scores))\n",
    "with open(osp.join(output_dir,'g.npy'), 'wb') as f:\n",
    "    np.save(f, np.array(g_scores))\n",
    "#with open('test.npy', 'rb') as f:\n",
    "#    a = np.load(f)\n",
    "print(osp.join(output_dir,'g.npy'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta=0\n",
      "Accuracy at errType=1:11582/15884= 0.7291614202971544\n",
      "Accuracy at errType=2:9111/15884= 0.5735960715185092\n",
      "Accuracy at errType=3:14568/15884= 0.917149332661798\n",
      "Accuracy for all types:35261/47652= 0.7399689414924872\n",
      "alpha= 0.0\n",
      "Accuracy at errType=1:14435/15884= 0.9087761269201713\n",
      "Accuracy at errType=2:12283/15884= 0.7732938806346008\n",
      "Accuracy at errType=3:12520/15884= 0.7882145555275749\n",
      "Accuracy for all types:39238/47652= 0.8234281876941156\n",
      "alpha= 0.1\n",
      "Accuracy at errType=1:14372/15884= 0.9048098715688744\n",
      "Accuracy at errType=2:12078/15884= 0.760387811634349\n",
      "Accuracy at errType=3:13438/15884= 0.846008562075044\n",
      "Accuracy for all types:39888/47652= 0.8370687484260891\n",
      "alpha= 0.2\n",
      "Accuracy at errType=1:14285/15884= 0.8993326617980357\n",
      "Accuracy at errType=2:11876/15884= 0.7476706119365399\n",
      "Accuracy at errType=3:13774/15884= 0.8671619239486276\n",
      "Accuracy for all types:39935/47652= 0.838055065894401\n",
      "alpha= 0.3\n",
      "Accuracy at errType=1:14182/15884= 0.8928481490808361\n",
      "Accuracy at errType=2:11627/15884= 0.731994459833795\n",
      "Accuracy at errType=3:13957/15884= 0.8786829513976329\n",
      "Accuracy for all types:39766/47652= 0.8345085201040879\n",
      "alpha= 0.4\n",
      "Accuracy at errType=1:14078/15884= 0.8863006799294888\n",
      "Accuracy at errType=2:11451/15884= 0.7209141274238227\n",
      "Accuracy at errType=3:14066/15884= 0.8855452027197179\n",
      "Accuracy for all types:39595/47652= 0.8309200033576765\n",
      "alpha= 0.5\n",
      "Accuracy at errType=1:13963/15884= 0.8790606900025183\n",
      "Accuracy at errType=2:11261/15884= 0.7089524049357845\n",
      "Accuracy at errType=3:14139/15884= 0.8901410224124906\n",
      "Accuracy for all types:39363/47652= 0.8260513724502644\n",
      "alpha= 0.6\n",
      "Accuracy at errType=1:13851/15884= 0.8720095693779905\n",
      "Accuracy at errType=2:11078/15884= 0.6974313774867792\n",
      "Accuracy at errType=3:14211/15884= 0.8946738856711156\n",
      "Accuracy for all types:39140/47652= 0.8213716108452951\n",
      "alpha= 0.7\n",
      "Accuracy at errType=1:13751/15884= 0.8657139259632335\n",
      "Accuracy at errType=2:10941/15884= 0.688806346008562\n",
      "Accuracy at errType=3:14245/15884= 0.896814404432133\n",
      "Accuracy for all types:38937/47652= 0.8171115588013095\n",
      "alpha= 0.8\n",
      "Accuracy at errType=1:13658/15884= 0.8598589775875094\n",
      "Accuracy at errType=2:10806/15884= 0.6803072273986401\n",
      "Accuracy at errType=3:14279/15884= 0.8989549231931503\n",
      "Accuracy for all types:38743/47652= 0.8130403760597666\n",
      "alpha= 0.9\n",
      "Accuracy at errType=1:13549/15884= 0.8529967262654243\n",
      "Accuracy at errType=2:10684/15884= 0.6726265424326366\n",
      "Accuracy at errType=3:14310/15884= 0.900906572651725\n",
      "Accuracy for all types:38543/47652= 0.8088432804499287\n",
      "alpha= 1.0\n",
      "Accuracy at errType=1:13456/15884= 0.8471417778897004\n",
      "Accuracy at errType=2:10583/15884= 0.666267942583732\n",
      "Accuracy at errType=3:14333/15884= 0.9023545706371191\n",
      "Accuracy for all types:38372/47652= 0.8052547637035171\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "no_errType = 3\n",
    "import numpy as np\n",
    "no_errType = 3\n",
    "for beta in [0]:\n",
    "    print(\"beta=0\")\n",
    "    cnt_corr_all = 0\n",
    "    cnt_incorr_all = 0\n",
    "    for errType in range(1,no_errType+1):\n",
    "        cnt_corr = 0\n",
    "        cnt_incorr = 0\n",
    "        for ita,g in zip(ita_scores,g_scores):\n",
    "            score = beta*np.array(ita)-np.array(g)\n",
    "            if score[0] > score[errType]:\n",
    "                cnt_corr +=1\n",
    "                cnt_corr_all +=1\n",
    "            else:\n",
    "                cnt_incorr +=1\n",
    "                cnt_incorr_all +=1\n",
    "        print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "    print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))\n",
    "\n",
    "for alpha in range(0,11):\n",
    "    alpha_ = alpha/10\n",
    "    print(\"alpha=\",alpha_)\n",
    "    cnt_corr_all = 0\n",
    "    cnt_incorr_all = 0\n",
    "    for errType in range(1,no_errType+1):\n",
    "        cnt_corr = 0\n",
    "        cnt_incorr = 0\n",
    "        for ita,g in zip(ita_scores,g_scores):\n",
    "            score = np.array(ita)-alpha_*np.array(g)\n",
    "            if score[0] > score[errType]:\n",
    "                cnt_corr +=1\n",
    "                cnt_corr_all +=1\n",
    "            else:\n",
    "                cnt_incorr +=1\n",
    "                cnt_incorr_all +=1\n",
    "        print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "    print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha here is γ in the paper.         \n",
    "#The selected value is 0.2\n",
    "#alpha= 0.2\n",
    "#Accuracy at errType=1:14285/15884= 0.8993326617980357\n",
    "#Accuracy at errType=2:11876/15884= 0.7476706119365399\n",
    "#Accuracy at errType=3:13774/15884= 0.8671619239486276\n",
    "#Accuracy for all types:39935/47652= 0.838055065894401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
