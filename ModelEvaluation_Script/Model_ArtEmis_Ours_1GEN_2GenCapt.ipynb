{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060cc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "#### Code adapted from the source code of ArtEmis dataset paper\n",
    "####################################################################\n",
    "Training a neural-speaker.\n",
    "\n",
    "The MIT License (MIT)\n",
    "Originally created at 6/16/20, for Python 3.x\n",
    "Copyright (c) 2021 Panos Achlioptas (ai.stanford.edu/~optas) & Stanford Geometric Computing Lab\n",
    "####################################################################\n",
    "\"\"\"\n",
    "import pprint\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "from torch import nn\n",
    "from termcolor import colored\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "    \n",
    "from model.argument import parse_test_speaker_arguments,set_seed\n",
    "from model.datasets_v2 import preprocess_dataset\n",
    "from model.func_train_v2 import load_state_dicts\n",
    "from model.func_test_v2 import read_saved_args,grounding_dataset_per_image_dummy,versatile_caption_sampler\n",
    "from model.func_test_v2 import pickle_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3d67cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters Specified:\n",
      "{'drop_bigrams': True,\n",
      " 'drop_unk': True,\n",
      " 'gpu': '0',\n",
      " 'img_dir': '',\n",
      " 'max_utterance_len': None,\n",
      " 'out_file': 'output/Ours_ArtEmis/CLIPViTB16_1Gen/fullDB_test_BS1.pkl',\n",
      " 'out_file_full': None,\n",
      " 'random_seed': 2021,\n",
      " 'sampling_config_file': 'None',\n",
      " 'speaker_checkpoint': 'output/Ours_ArtEmis/CLIPViTB16_1Gen/checkpoints/best_model.pt',\n",
      " 'speaker_saved_args': 'output/Ours_ArtEmis/CLIPViTB16_1Gen/config.json.txt',\n",
      " 'split': 'test'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelname = 'CLIPViTB16_1Gen' \n",
    "BS = 1\n",
    "\n",
    "model_dir = f'output/Ours_ArtEmis/{modelname}'\n",
    "outputfile = osp.join(model_dir,f'fullDB_test_BS{BS}.pkl')\n",
    "\n",
    "args_val = parse_test_speaker_arguments(\n",
    "        ['-speaker-saved-args',osp.join(model_dir,'config.json.txt'),\n",
    "         '-speaker-checkpoint',osp.join(model_dir,'checkpoints/best_model.pt'),#best_model.pt model_epoch_20.pt last_model\n",
    "         '-out-file',outputfile,\n",
    "         '-img-dir', '',\n",
    "         '--sampling-config-file', None,\n",
    "         '--split', 'test',\n",
    "         '--gpu','0']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5088945f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FreezeCase': 0,\n",
      " 'accum_iter': 2,\n",
      " 'backboneVisEnc': 'ViTB16',\n",
      " 'batch_size': 32,\n",
      " 'context_length': 65,\n",
      " 'data_dir': '../Dataset/ArtEmis/ArtEmis',\n",
      " 'debug': False,\n",
      " 'droprate': 0.0,\n",
      " 'gpu': '0',\n",
      " 'image_resolution': 224,\n",
      " 'img_dir': '../Dataset/ArtEmis/ArtEmis/../OriginalArtEmis/Images/CLIP_224',\n",
      " 'lr_others': 0.001,\n",
      " 'lr_patience': 2,\n",
      " 'lr_textEnc': 0.001,\n",
      " 'lr_visEnc': 1e-07,\n",
      " 'max_epochs': 200,\n",
      " 'modeltype': '1Gen',\n",
      " 'no_transform': True,\n",
      " 'output_dir': 'output/Ours_ArtEmis/CLIPViTB16_1Gen',\n",
      " 'random_seed': 2021,\n",
      " 'save_each_epoch': False,\n",
      " 'train_patience': 5,\n",
      " 'transformer_heads': 8,\n",
      " 'transformer_layers': 8,\n",
      " 'use_timestamp': False,\n",
      " 'use_vocabFAM': True,\n",
      " 'vocab_size': 15018}\n"
     ]
    }
   ],
   "source": [
    "args = read_saved_args(args_val.speaker_saved_args)\n",
    "modeltype = args.modeltype \n",
    "use_vocabFAM = args.use_vocabFAM\n",
    "print(pprint.pformat(vars(args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acceaa23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huyentran/anaconda3/envs/pytorch13/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (4,5,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 439135 captions!!!\n"
     ]
    }
   ],
   "source": [
    "## Load dataset\n",
    "file_name = 'ArtEmis.csv'\n",
    "df = pd.read_csv(osp.join(args.data_dir, file_name))\n",
    "df = df.where(pd.notnull(df), 'None')\n",
    "if args.random_seed != -1:\n",
    "    set_seed(args.random_seed)\n",
    "if args.debug:\n",
    "    df = df.sample(50)\n",
    "print(f'Loaded {len(df)} captions!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eadb8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.no_transform:\n",
    "    img_transform = None\n",
    "else:\n",
    "    if 'CLIP' in modelname:\n",
    "        img_transform = Compose([\n",
    "            Resize(args.image_resolution, interpolation=BICUBIC),\n",
    "            CenterCrop(args.image_resolution),\n",
    "            lambda image: image.convert(\"RGB\"),\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "    elif 'INRN34' in modelname:\n",
    "        image_net_mean = [0.485, 0.456, 0.406]\n",
    "        image_net_std = [0.229, 0.224, 0.225]\n",
    "        resample_method = Image.LANCZOS\n",
    "        normalize = Normalize(mean=image_net_mean, std=image_net_std)\n",
    "        img_transform = Compose([Resize((args.image_resolution, args.image_resolution), resample_method),ToTensor(),normalize])\n",
    "    elif 'INViTB16'in modelname:\n",
    "        img_transform = Compose([\n",
    "            Resize(size=248, interpolation=BICUBIC, max_size=None, antialias=None),\n",
    "            CenterCrop(size=(224, 224)),\n",
    "            ToTensor(),\n",
    "            Normalize((0.5000, 0.5000, 0.5000), (0.5000, 0.5000, 0.5000))])\n",
    "    else:\n",
    "        raise ValueError(f\"Do not support model = {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc921a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeltype == 'full': \n",
    "    from model.model_v2 import fullArc as idcmodel\n",
    "elif modeltype == 'woSG': \n",
    "    from model.model_v2 import woSGArc as idcmodel \n",
    "elif modeltype == '1Gen': \n",
    "    from model.model_v2 import oneGenArc as idcmodel \n",
    "else:\n",
    "    raise ValueError(f\"Do not support modeltype = {modeltype}!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d123f44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 30, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_vocabFAM:\n",
    "    from model.vocabulary import Vocabulary\n",
    "    from model.func_test_v2 import get_highest_prob_capt as get_highest_prob_capt\n",
    "    vocab = Vocabulary.load(osp.join(args.data_dir, 'ArtEmis_Vocab.pkl'))\n",
    "    eos_token = 2\n",
    "    sos_token = 1\n",
    "    and_token = vocab('and')\n",
    "    unk_token  = vocab.unk\n",
    "else: # Use original vocabulary of CLIP\n",
    "    from model.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "    from model.func_test_v2 import get_highest_prob_capt_CLIP as get_highest_prob_capt\n",
    "    vocab = _Tokenizer()\n",
    "    eos_token = args.vocab_size-1 # eos_token is the last number\n",
    "    sos_token = args.vocab_size-2 #sos_token is the last second number\n",
    "    and_token = vocab.encode('and')[0]\n",
    "    unk_token  = []\n",
    "    df['tokens_encoded'] = df['CLIP_tokens']\n",
    "sos_token,eos_token,and_token, unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a808da84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_transforms: {'train': None, 'val': None, 'test': None}\n",
      "Will use 348197 annotations for training.\n",
      "Will use 32011 annotations for validation.\n",
      "Will use 58927 annotations for testing.\n"
     ]
    }
   ],
   "source": [
    "df.tokens_encoded = df.tokens_encoded.apply(literal_eval)\n",
    "df.subject_encoded = df.subject_encoded.apply(literal_eval)\n",
    "df.predicate_encoded = df.predicate_encoded.apply(literal_eval)\n",
    "\n",
    "data_loaders, _ = preprocess_dataset(df, args,img_transform)\n",
    "print('Will use {} annotations for training.'.format(len(data_loaders['train'].dataset)))\n",
    "print('Will use {} annotations for validation.'.format(len(data_loaders['val'].dataset)))\n",
    "print('Will use {} annotations for testing.'.format(len(data_loaders['test'].dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1738b002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "Index(['image_files', 'tokens_encoded', 'subject_encoded',\n",
      "       'predicate_encoded'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "working_data_loader = data_loaders[args_val.split]\n",
    "if args_val.max_utterance_len is None:\n",
    "    # use the maximum length in the underlying split.\n",
    "    def utterance_len(tokens, eos_token=eos_token):\n",
    "        return np.where(np.asarray(tokens) == eos_token)[0][0] -1 # -1 to remove sos\n",
    "    args_val.max_utterance_len = working_data_loader.dataset.tokens_encoded.apply(utterance_len).max()\n",
    "    print(args_val.max_utterance_len)\n",
    "annotate_loader = grounding_dataset_per_image_dummy(working_data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1887bcfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Describe model\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:\" + str(args.gpu)) # CHECK HERE\n",
    "    \n",
    "model = idcmodel(args.backboneVisEnc,args.image_resolution,\n",
    "                args.context_length,args.vocab_size,sos_token,eos_token,\n",
    "                args.transformer_heads,args.transformer_layers,\n",
    "                args.droprate)\n",
    "\n",
    "loaded_epoch = load_state_dicts(args_val.speaker_checkpoint, map_location='cpu', model=model)\n",
    "model.to(device)\n",
    "loaded_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ae0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \"sampling_rule\": \"beam\",\"temperature\": 1.0,\"beam_size\": BS,'max_utterance_len':63, \n",
    "          'drop_unk':True, 'drop_bigrams':True}\n",
    "final_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265f1d01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling with configuration:  {'sampling_rule': 'beam', 'temperature': 1.0, 'beam_size': 1, 'max_utterance_len': 63, 'drop_unk': True, 'drop_bigrams': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5497/5497 [24:51<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Sampling with configuration: ', config)\n",
    "\n",
    "if args.random_seed != -1:\n",
    "    set_seed(args.random_seed)\n",
    "\n",
    "gen_df = versatile_caption_sampler(model,modeltype, annotate_loader,vocab,\n",
    "                                                    device, sos_token,eos_token,and_token, unk_token,\n",
    "                                                    args.vocab_size,**config)\n",
    "    \n",
    "final_results.append([config, gen_df])\n",
    "print('Done.')\n",
    "pickle_data(args_val.out_file, final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12f8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
