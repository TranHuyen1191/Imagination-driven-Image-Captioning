{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3eb1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Code adapted from the source code of ArtEmis dataset paper\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from model.func_eval import unpickle_data,torch_load_model\n",
    "from artemis.utils.vocabulary import Vocabulary #Use for text2emotion metrics\n",
    "from artemis.evaluation.single_caption_per_image import apply_basic_evaluations\n",
    "from artemis.emotions import IDX_TO_EMOTION\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8475c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBdir = \"../Dataset/ArtEmis/ArtEmis\"\n",
    "modelname = 'CLIPViTB16_woSG' #'CLIPViTB16_full','CLIPViTB16_woSG','INRN34_full','INRN34_woSG','INViTB16_full','INViTB16_woSG'\n",
    "\n",
    "model_dir = f'output/Ours_ArtEmis/{modelname}'\n",
    "sampled_captions_file = osp.join(model_dir,'fullDB_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5520a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_file = osp.join(DBdir,'Artemis_GT.pkl')\n",
    "split = 'test'\n",
    "gpu_id = 0\n",
    "\n",
    "# the evaluation of the longest-common-subsequence is quite slow -- so we sub-sampled the data:\n",
    "default_lcs_sample = [25000, 800]\n",
    "# First integer (25000) = number of training (gt) sentences to subsample from all training\n",
    "# Secong integer (800)  = number of sample sentences to subsample from all generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9a7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cider', 'spice', 'meteor', 'bleu', 'rouge'}\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "evaluation_methods = {'bleu', 'meteor', 'rouge', 'spice','cider'}\n",
    "print(evaluation_methods) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccdf086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.count_IdC import count_IdC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105f4499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on ArtEmis...\n",
      "Training Utterances 348197\n",
      "Images Captioned 5497\n",
      "BLEU: done\n",
      "COCO-based-metrics: done\n",
      "Using combined types!\n",
      "   metric      mean       std\n",
      "0  BLEU-0  0.618207  0.187845\n",
      "1  BLEU-1  0.380873  0.235182\n",
      "2  BLEU-2  0.224721  0.218318\n",
      "3  BLEU-3  0.135761  0.158792\n",
      "4   CIDER  0.112374  0.140179\n",
      "5   SPICE  0.064493  0.050768\n",
      "6  METEOR  0.158707  0.063991\n",
      "7   ROUGE  0.332289  0.110967\n",
      "Number of Id-Captions: 4140\n",
      "Number of unique Id-Captions: 2736\n",
      "Number of literal Captions: 1357\n",
      "Number of unique literal Captions: 1126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on ArtEmis...\")\n",
    "gt_data = next(unpickle_data(references_file))\n",
    "train_utters = gt_data['train']['references_pre_vocab']\n",
    "train_utters = list(itertools.chain(*train_utters))  # undo the grouping per artwork to a single large list\n",
    "print('Training Utterances', len(train_utters))\n",
    "unique_train_utters = set(train_utters)\n",
    "gt_data = gt_data[split]\n",
    "print('Images Captioned', len(gt_data))\n",
    "\n",
    "saved_samples = next(unpickle_data(sampled_captions_file))\n",
    "for sampling_config_details, captions in saved_samples:  # you might have sampled under several sampling configurations\n",
    "    merged = pd.merge(gt_data, captions)  # this ensures proper order of captions to gt (via accessing merged.captions)\n",
    "    merged['caption'] = merged.captions_predicted\n",
    "    hypothesis = merged.caption\n",
    "    references = merged.references_pre_vocab # i.e., use references that do not have <UNK>\n",
    "\n",
    "    metrics_eval = apply_basic_evaluations(hypothesis, references, None, None, None, \n",
    "                                           nltk_bleu=False, lcs_sample=default_lcs_sample,\n",
    "                                           train_utterances=unique_train_utters,\n",
    "                                           methods_to_do=evaluation_methods)\n",
    "    print(\"Using combined types!\")\n",
    "    print(pd.DataFrame(metrics_eval))\n",
    "    merged_IdC,merged_LC_df = count_IdC(merged)\n",
    "    print(\"Number of Id-Captions:\",len(merged_IdC))\n",
    "    print(\"Number of unique Id-Captions:\",len(set(merged_IdC.caption.tolist())))\n",
    "    print(\"Number of literal Captions:\",len(merged_LC_df))\n",
    "    print(\"Number of unique literal Captions:\",len(set(merged_LC_df.caption.tolist())))\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c1368e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on ArtEmis literal captions ...\n",
      "Training Utterances 272688\n",
      "Images Captioned 4019\n",
      "BLEU: done\n",
      "COCO-based-metrics: done\n",
      "   metric      mean       std\n",
      "0  BLEU-0  0.612091  0.180901\n",
      "1  BLEU-1  0.365989  0.216010\n",
      "2  BLEU-2  0.199232  0.183790\n",
      "3  BLEU-3  0.100891  0.102680\n",
      "4   CIDER  0.125504  0.152294\n",
      "5   SPICE  0.074605  0.054700\n",
      "6  METEOR  0.153407  0.061447\n",
      "7   ROUGE  0.334211  0.101582\n",
      "Number of Id-Captions: 0\n",
      "Number of unique Id-Captions: 0\n",
      "Number of literal Captions: 4019\n",
      "Number of unique literal Captions: 3180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on ArtEmis literal captions ...\")\n",
    "references_file = osp.join(DBdir,'Artemis_GT_LC.pkl')\n",
    "gt_data = next(unpickle_data(references_file))\n",
    "train_utters = gt_data['train']['references_pre_vocab']\n",
    "train_utters = list(itertools.chain(*train_utters))  # undo the grouping per artwork to a single large list\n",
    "print('Training Utterances', len(train_utters))\n",
    "unique_train_utters = set(train_utters)\n",
    "gt_data = gt_data[split]\n",
    "print('Images Captioned', len(gt_data))\n",
    "\n",
    "merged = pd.merge(gt_data, captions)  # this ensures proper order of captions to gt (via accessing merged.captions)\n",
    "merged['caption'] = merged.LC_predicted\n",
    "hypothesis = merged.caption\n",
    "references = merged.references_pre_vocab # i.e., use references that do not have <UNK>\n",
    "\n",
    "metrics_eval = apply_basic_evaluations(hypothesis, references, None, None, None, \n",
    "                                       nltk_bleu=False, lcs_sample=default_lcs_sample,\n",
    "                                       train_utterances=unique_train_utters,\n",
    "                                       methods_to_do=evaluation_methods)\n",
    "print(pd.DataFrame(metrics_eval))\n",
    "merged_IdC,merged_LC_df = count_IdC(merged)\n",
    "print(\"Number of Id-Captions:\",len(merged_IdC))\n",
    "print(\"Number of unique Id-Captions:\",len(set(merged_IdC.caption.tolist())))\n",
    "print(\"Number of literal Captions:\",len(merged_LC_df))\n",
    "print(\"Number of unique literal Captions:\",len(set(merged_LC_df.caption.tolist())))\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40e13f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on ArtEmis Id-captions ...\n",
      "Training Utterances 75509\n",
      "Images Captioned 2497\n",
      "BLEU: done\n",
      "COCO-based-metrics: done\n",
      "Using only IdC!\n",
      "   metric      mean       std\n",
      "0  BLEU-0  0.626189  0.172612\n",
      "1  BLEU-1  0.430129  0.190077\n",
      "2  BLEU-2  0.290601  0.218220\n",
      "3  BLEU-3  0.199041  0.190423\n",
      "4   CIDER  0.120872  0.162590\n",
      "5   SPICE  0.065644  0.056398\n",
      "6  METEOR  0.190781  0.061789\n",
      "7   ROUGE  0.376010  0.114564\n",
      "Number of Id-Captions: 2494\n",
      "Number of unique Id-Captions: 1883\n",
      "Number of literal Captions: 3\n",
      "Number of unique literal Captions: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on ArtEmis Id-captions ...\")\n",
    "references_file = osp.join(DBdir,'Artemis_GT_IdC.pkl')\n",
    "gt_data = next(unpickle_data(references_file))\n",
    "train_utters = gt_data['train']['references_pre_vocab']\n",
    "train_utters = list(itertools.chain(*train_utters))  # undo the grouping per artwork to a single large list\n",
    "print('Training Utterances', len(train_utters))\n",
    "unique_train_utters = set(train_utters)\n",
    "gt_data = gt_data[split]\n",
    "print('Images Captioned', len(gt_data))\n",
    "\n",
    "merged = pd.merge(gt_data, captions)  # this ensures proper order of captions to gt (via accessing merged.captions)\n",
    "merged['caption'] = merged.IdC_predicted\n",
    "hypothesis = merged.caption\n",
    "references = merged.references_pre_vocab # i.e., use references that do not have <UNK>\n",
    "metrics_eval = apply_basic_evaluations(hypothesis, references, None, None, None, \n",
    "                                           nltk_bleu=False, lcs_sample=default_lcs_sample,\n",
    "                                           train_utterances=unique_train_utters,\n",
    "                                           methods_to_do=evaluation_methods)\n",
    "print(\"Using only IdC!\")\n",
    "print(pd.DataFrame(metrics_eval))\n",
    "merged_IdC,merged_LC_df = count_IdC(merged)\n",
    "print(\"Number of Id-Captions:\",len(merged_IdC))\n",
    "print(\"Number of unique Id-Captions:\",len(set(merged_IdC.caption.tolist())))\n",
    "print(\"Number of literal Captions:\",len(merged_LC_df))\n",
    "print(\"Number of unique literal Captions:\",len(set(merged_LC_df.caption.tolist())))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75522221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa29fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
