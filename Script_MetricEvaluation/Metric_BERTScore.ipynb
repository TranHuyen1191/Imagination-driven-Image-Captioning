{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caption sets in the test set: 15884\n"
     ]
    }
   ],
   "source": [
    "## Prepare the  dataset (merge it with the emotion-histograms.)\n",
    "datasetname = 'ArtEmis' #ArtEmis, Flickr30K,  VizWiz, COCO\n",
    "assert datasetname == 'ArtEmis' ## Only work with ArtEmis\n",
    "if datasetname == 'ArtEmis':\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdC/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/{datasetname}_IdC/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "    df = df[df.split=='test']\n",
    "else:\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print('Number of caption sets in the test set:', len(df))\n",
    "df.img_files = [osp.join(img_dir,imgfile) for imgfile in df.img_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['captSet_text'] = df['captSet_text'].apply(literal_eval)\n",
    "df['refCaptSet'] = df['refCaptSet'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_imgs =len(df)\n",
    "cands = []\n",
    "refs = []\n",
    "for _,row in df.iterrows():\n",
    "    refs_ =row['refCaptSet']\n",
    "    for cand in row['captSet_text']:\n",
    "        cands.append(cand)\n",
    "        refs.append(refs_)\n",
    "    \n",
    "scores_P, scores_R, scores_F1 = scorer.score(cands, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_P = np.array(scores_P).reshape([-1,len(row['captSet_text'])])\n",
    "scores_R = np.array(scores_R).reshape([-1,len(row['captSet_text'])])\n",
    "scores_F1 = np.array(scores_F1).reshape([-1,len(row['captSet_text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ArtEmis ,Number of images: 15884\n",
      "scores_P\n",
      "Accuracy at errType=1:12666/15884= 0.7974061949131201\n",
      "Accuracy at errType=2:10413/15884= 0.6555653487786451\n",
      "Accuracy at errType=3:8851/15884= 0.557227398640141\n",
      "Accuracy for all types:31930/47652= 0.6700663141106354\n",
      "Dataset: ArtEmis ,Number of images: 15884\n",
      "scores_R\n",
      "Accuracy at errType=1:11137/15884= 0.7011458071014858\n",
      "Accuracy at errType=2:11556/15884= 0.7275245530093175\n",
      "Accuracy at errType=3:12644/15884= 0.7960211533618736\n",
      "Accuracy for all types:35337/47652= 0.7415638378242256\n",
      "Dataset: ArtEmis ,Number of images: 15884\n",
      "scores_F1\n",
      "Accuracy at errType=1:12348/15884= 0.7773860488541929\n",
      "Accuracy at errType=2:11184/15884= 0.7041047595064216\n",
      "Accuracy at errType=3:10509/15884= 0.6616091664568119\n",
      "Accuracy for all types:34041/47652= 0.7143666582724755\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "no_errType = 3\n",
    "cnt_corr_all = 0\n",
    "cnt_incorr_all = 0\n",
    "print(\"Dataset:\",datasetname,\", Number of caption sets:\",len(scores_P))\n",
    "print(\"scores_P\")\n",
    "for errType in range(1,no_errType+1):\n",
    "    cnt_corr = 0\n",
    "    cnt_incorr = 0\n",
    "    for sim in scores_P:\n",
    "        if sim[0] > sim[errType]:\n",
    "            cnt_corr +=1\n",
    "            cnt_corr_all +=1\n",
    "        else:\n",
    "            cnt_incorr +=1\n",
    "            cnt_incorr_all +=1\n",
    "    print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))\n",
    "\n",
    "cnt_corr_all = 0\n",
    "cnt_incorr_all = 0\n",
    "print(\"Dataset:\",datasetname,\", Number of caption sets:\",len(scores_R))\n",
    "print(\"scores_R\")\n",
    "for errType in range(1,no_errType+1):\n",
    "    cnt_corr = 0\n",
    "    cnt_incorr = 0\n",
    "    for sim in scores_R:\n",
    "        if sim[0] > sim[errType]:\n",
    "            cnt_corr +=1\n",
    "            cnt_corr_all +=1\n",
    "        else:\n",
    "            cnt_incorr +=1\n",
    "            cnt_incorr_all +=1\n",
    "    print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))\n",
    "\n",
    "cnt_corr_all = 0\n",
    "cnt_incorr_all = 0\n",
    "print(\"Dataset:\",datasetname,\", Number of caption sets:\",len(scores_F1))\n",
    "print(\"scores_F1\")\n",
    "for errType in range(1,no_errType+1):\n",
    "    cnt_corr = 0\n",
    "    cnt_incorr = 0\n",
    "    for sim in scores_F1:\n",
    "        if sim[0] > sim[errType]:\n",
    "            cnt_corr +=1\n",
    "            cnt_corr_all +=1\n",
    "        else:\n",
    "            cnt_incorr +=1\n",
    "            cnt_incorr_all +=1\n",
    "    print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
