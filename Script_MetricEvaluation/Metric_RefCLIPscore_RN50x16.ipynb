{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pdb\n",
    "import clip \n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caption sets in the test set: 1699\n"
     ]
    }
   ],
   "source": [
    "## Prepare the  dataset (merge it with the emotion-histograms.)\n",
    "datasetname = 'COCO' #ArtEmis, Flickr30K,  VizWiz, COCO\n",
    "\n",
    "if datasetname == 'ArtEmis':\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdC/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/{datasetname}_IdC/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "    df = df[df.split=='test']\n",
    "else:\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print('Number of caption sets in the test set:', len(df))\n",
    "df['captSet_CLIP_tokens'] = df['captSet_CLIP_tokens'].apply(literal_eval)\n",
    "df.img_files = [osp.join(img_dir,imgfile) for imgfile in df.img_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_name = 'RN50x16'\n",
    "CLIPmodel,CLIPtransform,CLIPsettings = clip.load(CLIP_name,device, jit=False)\n",
    "embed_dim,image_resolution, vision_layers, vision_width, vision_patch_size,context_length_CLIP, vocab_size_CLIP, transformer_width, transformer_heads, transformer_layers = CLIPsettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['refCaptSet_CLIP_tokens'] = df['refCaptSet_CLIP_tokens'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'A photo depicts '\n",
    "prefix_encoded = clip.tokenize(prefix)[0]\n",
    "prefix_encoded  = prefix_encoded[torch.nonzero(prefix_encoded)].reshape(-1)[:-1].tolist()\n",
    "\n",
    "def preprocess_dataset(df,img_dim):\n",
    "    img_transform = Compose([ \n",
    "                        Resize(image_resolution, interpolation=BICUBIC),\n",
    "                        CenterCrop(image_resolution),\n",
    "                        lambda image: image.convert(\"RGB\"),\n",
    "                        ToTensor(),\n",
    "                        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "                    ])\n",
    "    dataset = Dataset(df.img_files, df.captSet_CLIP_tokens,df.refCaptSet_CLIP_tokens,img_transform=img_transform)\n",
    "    return dataset\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, image_files,captSets,refCaptSet,img_transform=None):\n",
    "        super(Dataset).__init__()\n",
    "        self.image_files = image_files\n",
    "        self.captSets = captSets\n",
    "        self.refCaptSet = refCaptSet\n",
    "        self.img_transform = img_transform\n",
    "        self.no_tokens = len(self.captSets[0][0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        captSet = []\n",
    "        \n",
    "        for capt in self.captSets[index]:\n",
    "            prefix_capt = prefix_encoded +capt[1:]\n",
    "            captSet.append(prefix_capt[:self.no_tokens]) \n",
    "        captSet = np.array(captSet).astype(dtype=np.long)\n",
    "        refCaptSet = np.array(captSet).astype(dtype=np.long)\n",
    "        if self.image_files is not None:\n",
    "            img = Image.open(self.image_files[index])\n",
    "\n",
    "            if img.mode is not 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            if self.img_transform is not None:\n",
    "                img = self.img_transform(img)\n",
    "        else:\n",
    "            img = []\n",
    "        item = {'image': img, 'captSet': captSet,'refCaptSet': refCaptSet, 'index': index}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captSets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracting CLIPscore using code from https://github.com/jmhessel/clipscore\n",
    "\"\"\"\n",
    "from packaging import version\n",
    "import sklearn.preprocessing\n",
    "CLIPmodel.eval()\n",
    "w=2.5\n",
    "\n",
    "dataset = preprocess_dataset(df,image_resolution)\n",
    "refclipscores = []\n",
    "no_imgs =len(dataset)\n",
    "print(no_imgs)\n",
    "\n",
    "for i in range(no_imgs):\n",
    "    data = dataset[i]   \n",
    "    image_inputs = torch.unsqueeze(data['image'], 0)\n",
    "    text_inputs  = torch.LongTensor(data['captSet']) \n",
    "    refText_inputs  = torch.LongTensor(data['refCaptSet']) \n",
    "    # Calculate features\n",
    "    with torch.no_grad():\n",
    "        image_inputs = image_inputs.to(device)\n",
    "        if device == 'cuda':\n",
    "            image_inputs = image_inputs.to(torch.float16)\n",
    "                \n",
    "        image_features = CLIPmodel.encode_image(image_inputs).cpu().numpy()\n",
    "        text_features = CLIPmodel.encode_text(text_inputs.to(device)).cpu().numpy()\n",
    "        if len(data['refCaptSet']) > 0:\n",
    "            refText_features = CLIPmodel.encode_text(refText_inputs.to(device)).cpu().numpy()\n",
    "        \n",
    "        \n",
    "    #as of numpy 1.21, normalize doesn't work properly for float16\n",
    "    if version.parse(np.__version__) < version.parse('1.21'):\n",
    "        image_features = sklearn.preprocessing.normalize(image_features, axis=1)\n",
    "        text_features = sklearn.preprocessing.normalize(text_features, axis=1)\n",
    "        if len(data['refCaptSet']) > 0:\n",
    "            refText_features = sklearn.preprocessing.normalize(refText_features, axis=1)\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            'due to a numerical instability, new numpy normalization is slightly different than paper results. '\n",
    "            'to exactly replicate paper results, please use numpy version less than 1.21, e.g., 1.20.3.')\n",
    "        image_features = image_features / np.sqrt(np.sum(image_features**2, axis=1, keepdims=True))\n",
    "        text_features = text_features / np.sqrt(np.sum(text_features**2, axis=1, keepdims=True))\n",
    "        if len(data['refCaptSet']) > 0:\n",
    "            refText_features = refText_features / np.sqrt(np.sum(refText_features**2, axis=1, keepdims=True))\n",
    "    \n",
    "    per_instance_image_text = w*np.clip(np.sum(image_features * text_features, axis=1), 0, None)\n",
    "    \n",
    "    if len(data['refCaptSet']) > 0:\n",
    "        per_instance_text_text = []\n",
    "        for text_feat in text_features:\n",
    "            per_instance_text_text.append(np.max(text_feat.dot(refText_features.transpose())))\n",
    "        score = 2 * per_instance_image_text * per_instance_text_text / (per_instance_image_text + per_instance_text_text)\n",
    "    else:\n",
    "        score = per_instance_image_text\n",
    "    refclipscores.append(score.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: COCO , Number of caption sets: 1699\n",
      "Accuracy at errType=1:1645/1699= 0.9682165979988229\n",
      "Accuracy at errType=2:1509/1699= 0.8881695114773396\n",
      "Accuracy at errType=3:1345/1699= 0.7916421424367275\n",
      "Accuracy for all types:4499/5097= 0.8826760839709633\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "no_errType = 3\n",
    "cnt_corr_all = 0\n",
    "cnt_incorr_all = 0\n",
    "print(\"Dataset:\",datasetname,\", Number of caption sets:\",len(refclipscores))\n",
    "for errType in range(1,no_errType+1):\n",
    "    cnt_corr = 0\n",
    "    cnt_incorr = 0\n",
    "    for sim in refclipscores:\n",
    "        if sim[0] > sim[errType]:\n",
    "            cnt_corr +=1\n",
    "            cnt_corr_all +=1\n",
    "        else:\n",
    "            cnt_incorr +=1\n",
    "            cnt_incorr_all +=1\n",
    "    print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-79af7ddd001d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-79af7ddd001d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Dataset: COCO , Number of caption sets: 1699\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Dataset: COCO , Number of caption sets: 1699\n",
    "Accuracy at errType=1:1645/1699= 0.9682165979988229\n",
    "Accuracy at errType=2:1509/1699= 0.8881695114773396\n",
    "Accuracy at errType=3:1345/1699= 0.7916421424367275\n",
    "Accuracy for all types:4499/5097= 0.8826760839709633\n",
    "\n",
    "Dataset: VizWiz , Number of caption sets: 1160\n",
    "Accuracy at errType=1:1008/1160= 0.8689655172413793\n",
    "Accuracy at errType=2:1016/1160= 0.8758620689655172\n",
    "Accuracy at errType=3:788/1160= 0.6793103448275862\n",
    "Accuracy for all types:2812/3480= 0.8080459770114943\n",
    "\n",
    "Dataset: Flickr30K , Number of caption sets: 595\n",
    "Accuracy at errType=1:575/595= 0.9663865546218487\n",
    "Accuracy at errType=2:531/595= 0.892436974789916\n",
    "Accuracy at errType=3:380/595= 0.6386554621848739\n",
    "Accuracy for all types:1486/1785= 0.8324929971988796\n",
    "\n",
    "Dataset: ArtEmis , Number of caption sets: 15884\n",
    "Accuracy at errType=1:12386/15884= 0.7797783933518005\n",
    "Accuracy at errType=2:12157/15884= 0.7653613699320071\n",
    "Accuracy at errType=3:8696/15884= 0.5474691513472677\n",
    "Accuracy for all types:33239/47652= 0.6975363048770251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
