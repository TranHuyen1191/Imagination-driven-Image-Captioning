{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caption sets in the test set: 1699\n"
     ]
    }
   ],
   "source": [
    "## Prepare the  dataset (merge it with the emotion-histograms.)\n",
    "datasetname = 'COCO' #ArtEmis, Flickr30K,  VizWiz, COCO\n",
    "\n",
    "if datasetname == 'ArtEmis':\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdC/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/{datasetname}_IdC/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "    df = df[df.split=='test']\n",
    "else:\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print('Number of caption sets in the test set:', len(df))\n",
    "df['captSet_CLIP_tokens'] = df['captSet_CLIP_tokens'].apply(literal_eval)\n",
    "df.img_files = [osp.join(img_dir,imgfile) for imgfile in df.img_files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar to make_txt_db.py in UMIC source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['captSet_text'] = df['captSet_text'].apply(literal_eval)\n",
    "no_errType = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from cytoolz import curry\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "@curry\n",
    "def bert_tokenize(tokenizer, text):\n",
    "    ids = []\n",
    "    for word in text.strip().split():\n",
    "        ws = tokenizer.tokenize(word)\n",
    "        if not ws:\n",
    "            # some special char\n",
    "            continue\n",
    "        ids.extend(tokenizer.convert_tokens_to_ids(ws))\n",
    "    return ids\n",
    "def invert_dict(d):\n",
    "    d_inv = defaultdict(list)\n",
    "    for k, v in d.items():\n",
    "        d_inv[v].append(k)\n",
    "    return d_inv\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "tokenizer = bert_tokenize(bert_tokenizer)\n",
    "\n",
    "meta = {\n",
    "    #'annotations': ['./default.jsonl'],\n",
    " #'output': './default',\n",
    " #'format': 'lmdb',\n",
    " 'task': 'caption_evaluation',\n",
    " 'bert': 'bert-base-cased',\n",
    " 'UNK': 100,\n",
    " 'CLS': 101,\n",
    " 'SEP': 102,\n",
    " 'MASK': 103,\n",
    " 'v_range': [106, 28996]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6796/6796 [00:00<00:00, 7649.46it/s]\n",
      "100%|██████████| 6796/6796 [00:00<00:00, 346319.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of captions: 1489\n"
     ]
    }
   ],
   "source": [
    "captions = []\n",
    "img_ids = []\n",
    "for _,row in df.iterrows():\n",
    "    imgfeat_file = row['imgfeat_file']\n",
    "    captSet = row['captSet_text']\n",
    "    for i in range(no_errType+1): # natural and unnatural captions\n",
    "        captions.append(captSet[i])\n",
    "        img_ids.append(imgfeat_file)\n",
    "        \n",
    "ce_txt2img = {}\n",
    "ce_id2len = {}\n",
    "sent_ids = []\n",
    "for i in range(len(img_ids)):\n",
    "    ce_txt2img[str(i)] = img_ids[i]\n",
    "ce_img2txts = dict(invert_dict(ce_txt2img))\n",
    "\n",
    "for i in tqdm(range(len(img_ids))):\n",
    "    sent = captions[i]\n",
    "    input_ids = tokenizer(sent)\n",
    "    sent_ids.append(input_ids)\n",
    "    ce_id2len[str(i)] = len(input_ids)\n",
    "db = {}   \n",
    "for i in tqdm(range(len(img_ids))):\n",
    "    id_ = str(i)\n",
    "    example = {}\n",
    "    sent = captions[i].lower()\n",
    "    example['input_ids'] = sent_ids[i]\n",
    "    example['img_fname'] = ce_txt2img[id_]\n",
    "    example['target'] = 1.0\n",
    "    db[id_] = example\n",
    "\n",
    "\n",
    "name2nbb = {}\n",
    "for file in list(set(img_ids)):\n",
    "    imgfeat = np.load(file, allow_pickle=True)\n",
    "    name2nbb[file] = int(imgfeat['num_bbox'])\n",
    "print(\"Number of captions:\",len(name2nbb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6796 6796 6796 6796 6796\n",
      "1489 1489\n"
     ]
    }
   ],
   "source": [
    "print(len(captions),len(img_ids),len(ce_txt2img),len(ce_id2len),len(db))\n",
    "print(len(ce_img2txts),len(name2nbb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar to compute_metric.py in UMIC source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from os.path import exists\n",
    "import pickle\n",
    "from time import time\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from horovod import torch as hvd\n",
    "\n",
    "from UMIC.data import (PrefetchLoader,\n",
    "                  DetectFeatLmdb_Tran, TxtTokLmdb_Tran, ItmEvalDataset, itm_eval_collate,\n",
    "                 CeEvalDataset, ce_eval_collate)\n",
    "from UMIC.model.ce import UniterForCaptioningMetric\n",
    "from UMIC.utils.distributed import all_gather_list\n",
    "from UMIC.utils.const import IMG_DIM\n",
    "from UMIC.utils.itm_eval import inference, itm_eval\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as scss\n",
    "from scipy import stats\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/10/2022 10:49:00 - INFO - UMIC.model.model -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hvd.init()\n",
    "n_gpu = hvd.size()\n",
    "device = torch.device(\"cuda\", hvd.local_rank())\n",
    "torch.cuda.set_device(hvd.local_rank())\n",
    "rank = hvd.rank()\n",
    "opts = SimpleNamespace(compressed_db=False, max_txt_len=60,conf_th=0.2, max_bb=100, min_bb=10, num_bb=36, inf_minibatch_size=400, margin=0.2,\n",
    "                      valid_steps=1000, n_workers=4, fp16=True,\n",
    "                      model_config='./UMIC/config/uniter-base.json',\n",
    "                      output_dir='output/UMIC_results',\n",
    "                      pin_mem=True,\n",
    "                      batch_size=128,\n",
    "                      checkpoint='UMIC/ckpt/umic.pt')\n",
    "# load DBs and image dirs\n",
    "eval_img_db = DetectFeatLmdb_Tran(name2nbb,opts.conf_th, opts.max_bb,\n",
    "                             opts.min_bb, opts.num_bb,\n",
    "                             opts.compressed_db)\n",
    "eval_txt_db = TxtTokLmdb_Tran( db,meta,ce_txt2img,ce_id2len, ce_img2txts,-1)\n",
    "eval_dataset = CeEvalDataset(eval_txt_db, eval_img_db)\n",
    "# Prepare model\n",
    "load_checkpoint = torch.load(opts.checkpoint)\n",
    "\n",
    "model = UniterForCaptioningMetric.from_pretrained(\n",
    "    opts.model_config, load_checkpoint, img_dim=IMG_DIM)\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=opts.batch_size, shuffle=False,\n",
    "                         num_workers=1,\n",
    "                         pin_memory=False,\n",
    "                         collate_fn=ce_eval_collate)\n",
    "eval_dataloader = PrefetchLoader(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:30,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of captions: 6796\n",
      "UMIC Score: 0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "umic_scores = []\n",
    "qids = []\n",
    "for i, batch, in tqdm(enumerate(eval_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        scores = model(batch, compute_loss=False)\n",
    "        umic_scores += (list(scores.squeeze().detach().cpu().numpy()))\n",
    "        qids += batch['qids']\n",
    "\n",
    "umic_scores = [sigmoid(x) for x in umic_scores]\n",
    "print(\"Number of captions:\",len(umic_scores))\n",
    "print(\"UMIC Score: %.3f\"% np.average(umic_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "umic_scores_dict = dict(zip(qids, umic_scores))\n",
    "umic_scores_full = []\n",
    "cnt = 0\n",
    "for _,row in df.iterrows():\n",
    "    umic_scores_temp = []\n",
    "    for i in range(0,no_errType+1): # natural and unnatural captions\n",
    "        umic_scores_temp.append(umic_scores_dict[str(cnt)])\n",
    "        cnt +=1\n",
    "    umic_scores_full.append(umic_scores_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: COCO , Number of caption sets: 1699\n",
      "Accuracy at errType=1:1670/1699= 0.9829311359623308\n",
      "Accuracy at errType=2:1544/1699= 0.9087698646262508\n",
      "Accuracy at errType=3:1410/1699= 0.8298999411418482\n",
      "Accuracy for all types:4624/5097= 0.9072003139101432\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "no_errType = 3\n",
    "cnt_corr_all = 0\n",
    "cnt_incorr_all = 0\n",
    "print(\"Dataset:\",datasetname,\", Number of caption sets:\",len(umic_scores_full))\n",
    "for errType in range(1,no_errType+1):\n",
    "    cnt_corr = 0\n",
    "    cnt_incorr = 0\n",
    "    for sim in umic_scores_full:\n",
    "        if sim[0] > sim[errType]:\n",
    "            cnt_corr +=1\n",
    "            cnt_corr_all +=1\n",
    "        else:\n",
    "            cnt_incorr +=1\n",
    "            cnt_incorr_all +=1\n",
    "    print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-50df44db92bf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-50df44db92bf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Dataset: COCO , Number of caption sets: 1699\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Dataset: COCO , Number of caption sets: 1699\n",
    "Accuracy at errType=1:1670/1699= 0.9829311359623308\n",
    "Accuracy at errType=2:1544/1699= 0.9087698646262508\n",
    "Accuracy at errType=3:1410/1699= 0.8298999411418482\n",
    "Accuracy for all types:4624/5097= 0.9072003139101432\n",
    "\n",
    "Dataset: VizWiz , Number of caption sets: 1160\n",
    "Accuracy at errType=1:993/1160= 0.8560344827586207\n",
    "Accuracy at errType=2:939/1160= 0.8094827586206896\n",
    "Accuracy at errType=3:798/1160= 0.6879310344827586\n",
    "Accuracy for all types:2730/3480= 0.7844827586206896\n",
    "\n",
    "Dataset: Flickr30K , Number of caption sets: 595\n",
    "Accuracy at errType=1:579/595= 0.973109243697479\n",
    "Accuracy at errType=2:531/595= 0.892436974789916\n",
    "Accuracy at errType=3:462/595= 0.7764705882352941\n",
    "Accuracy for all types:1572/1785= 0.880672268907563\n",
    "    \n",
    "Dataset: ArtEmis , Number of caption sets: 15884\n",
    "Accuracy at errType=1:11514/15884= 0.7248803827751196\n",
    "Accuracy at errType=2:11198/15884= 0.7049861495844876\n",
    "Accuracy at errType=3:9536/15884= 0.6003525560312264\n",
    "Accuracy for all types:32248/47652= 0.6767396961302778"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
