{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caption sets in the test set: 15884\n"
     ]
    }
   ],
   "source": [
    "## Prepare the  dataset (merge it with the emotion-histograms.)\n",
    "datasetname = 'ArtEmis' #ArtEmis, Flickr30K,  VizWiz, COCO\n",
    "assert datasetname == 'ArtEmis' ## Only work with ArtEmis\n",
    "if datasetname == 'ArtEmis':\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdC/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/{datasetname}_IdC/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "    df = df[df.split=='test']\n",
    "else:\n",
    "    datafile = f'../Dataset/{datasetname}/{datasetname}_IdCII_3ErrType.csv'\n",
    "    img_dir = f\"../Dataset/{datasetname}/Images/rawImages\"\n",
    "    df = pd.read_csv(datafile)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print('Number of caption sets in the test set:', len(df))\n",
    "df.img_files = [osp.join(img_dir,imgfile) for imgfile in df.img_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['captSet_text'] = df['captSet_text'].apply(literal_eval)\n",
    "df['refCaptSet'] = df['refCaptSet'].apply(literal_eval)\n",
    "img_feat_dir = 'output/feat_ViLBERTScore' # Extracted using ExtractImgFeature_ViLBERTScore.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/10/2022 11:05:06 - INFO - ViLBERTScore.vilbert.utils -   loading weights file ViLBERTScore/save/multi_task_model.bin\n"
     ]
    }
   ],
   "source": [
    "## Calculate ViLBERTscore based on https://github.com/hwanheelee1993/ViLBERTScore\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"vilbert.utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from ViLBERTScore.vilbert.datasets import ConceptCapLoaderTrain, ConceptCapLoaderVal\n",
    "from ViLBERTScore.vilbert.vilbert import VILBertForVLTasks, BertConfig, BertForMultiModalPreTraining\n",
    "from ViLBERTScore.vilbert.task_utils import LoadDatasetEval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import argparse\n",
    "import glob\n",
    "from types import SimpleNamespace\n",
    "import pdb\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import kendalltau\n",
    "from torch.nn.functional import softmax\n",
    "from utils import *\n",
    "from ViLBERTScore.dataset import CaptioningDataset\n",
    "from torch.utils.data import DataLoader\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased', do_lower_case=True\n",
    ")\n",
    "args = SimpleNamespace(from_pretrained= \"ViLBERTScore/save/multi_task_model.bin\",\n",
    "                       bert_model=\"bert-base-uncased\",\n",
    "                       config_file=\"ViLBERTScore/config/bert_base_6layer_6conect.json\",\n",
    "                       max_seq_length=101,\n",
    "                       train_batch_size=1,\n",
    "                       do_lower_case=True,\n",
    "                       predict_feature=False,\n",
    "                       seed=42,\n",
    "                       num_workers=0,\n",
    "                       baseline=False,\n",
    "                       img_weight=1,\n",
    "                       distributed=False,\n",
    "                       objective=1,\n",
    "                       visual_target=0,\n",
    "                       dynamic_attention=False,\n",
    "                       task_specific_tokens=True,\n",
    "                       tasks='1',\n",
    "                       save_name='',\n",
    "                       in_memory=False,\n",
    "                       batch_size=1,\n",
    "                       local_rank=-1,\n",
    "                       split='mteval',\n",
    "                       clean_train_sets=True,\n",
    "                       dataset=datasetname,\n",
    "                       task=7,\n",
    "                       layer=-1,\n",
    "                       expname='pretrain_cls_sep'\n",
    "                      )\n",
    "args.task = 7\n",
    "args.layer = -1\n",
    "args.from_pretrained = 'ViLBERTScore/save/multi_task_model.bin'\n",
    "args.compute_correlation = False\n",
    "\n",
    "if(args.from_pretrained == 'ViLBERTScore/save/pretrained_model.bin'):\n",
    "    args.task_specific_tokens = False\n",
    "\n",
    "config = BertConfig.from_json_file(args.config_file)\n",
    "with open('ViLBERTScore/vilbert_tasks.yml', 'r') as f:\n",
    "    task_cfg = edict(yaml.safe_load(f))\n",
    "\n",
    "\n",
    "task_names = []\n",
    "for i, task_id in enumerate(args.tasks.split('-')):\n",
    "    task = 'TASK' + task_id\n",
    "    name = task_cfg[task]['name']\n",
    "    task_names.append(name)\n",
    "\n",
    "timeStamp = args.from_pretrained.split('/')[-1] + '-' + args.save_name\n",
    "config = BertConfig.from_json_file(args.config_file)\n",
    "default_gpu=True\n",
    "\n",
    "if args.predict_feature:\n",
    "    config.v_target_size = 2048\n",
    "    config.predict_feature = True\n",
    "else:\n",
    "    config.v_target_size = 1601\n",
    "    config.predict_feature = False\n",
    "\n",
    "if args.task_specific_tokens:\n",
    "    config.task_specific_tokens = True    \n",
    "\n",
    "if args.dynamic_attention:\n",
    "    config.dynamic_attention = True\n",
    "\n",
    "config.visualization = True\n",
    "num_labels = 3129\n",
    "\n",
    "if args.baseline:\n",
    "    model = BaseBertForVLTasks.from_pretrained(\n",
    "        args.from_pretrained, config=config, num_labels=num_labels, default_gpu=default_gpu\n",
    "        )\n",
    "else:\n",
    "    model = VILBertForVLTasks.from_pretrained(\n",
    "        args.from_pretrained, config=config, num_labels=num_labels, default_gpu=default_gpu\n",
    "        )\n",
    "    \n",
    "model.eval()\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda: model = model.cuda(0)\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")\n",
    "from ViLBERTScore.utils import *\n",
    "def compute_bert_score(model, text_a, input_mask_a, segment_ids_a, \n",
    "                       features, spatials, image_mask, co_attention_mask, input_idf_a, task, x, y, \n",
    "                       use_idf=False, layer=-1):\n",
    "    with torch.no_grad():\n",
    "        p_allRef = []\n",
    "        r_allRef = []\n",
    "        f_allRef = []            \n",
    "\n",
    "        st_c, sv_c, pt_c, pv_c, att_c = model.bert(\n",
    "            text_a[:,x,:], features, spatials, segment_ids_a[:,x,:],\n",
    "            input_mask_a[:,x,:], image_mask, co_attention_mask, task,\n",
    "        output_all_encoded_layers=True)\n",
    "        for i in range(text_a.size(1)-y):#range(5):\n",
    "            st_g, sv_g, pt_g, pv_g, att_g = model.bert(\n",
    "                text_a[:,y+i,:], features, spatials, segment_ids_a[:,y+i,:],\n",
    "                input_mask_a[:,y+i,:], image_mask, co_attention_mask, task,\n",
    "            output_all_encoded_layers=True)\n",
    "            if(use_idf):\n",
    "                p, r, f = bert_score(st_g[layer], st_c[layer], input_idf_a[:,y+i, :], input_idf_a[:,x, :])\n",
    "            else:\n",
    "                p, r, f = bert_score(st_g[layer], st_c[layer], input_mask_a[:,y+i, :], input_mask_a[:,x, :])\n",
    "            p_allRef.append(p)\n",
    "            r_allRef.append(r)\n",
    "            f_allRef.append(f)        \n",
    "\n",
    "    p_allRefa = np.average(p_allRef, axis=0)\n",
    "    r_allRefa = np.average(r_allRef, axis=0)\n",
    "    f_allRefa = np.average(f_allRef, axis=0) \n",
    "    \n",
    "    return p_allRefa, r_allRefa, f_allRefa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63536/63536 [4:07:27<00:00,  4.28it/s]  \n"
     ]
    }
   ],
   "source": [
    "no_imgs =len(df)\n",
    "cands = []\n",
    "refs = []\n",
    "imgfeatfiles = []\n",
    "for _,row in df.iterrows():\n",
    "    refs_ =row['refCaptSet']\n",
    "    for cand in row['captSet_text']:\n",
    "        cands.append(cand)\n",
    "        refs.append(refs_)\n",
    "        imgfeatfiles.append(osp.join(img_feat_dir, row.art_style,  row.painting + '.npy')) \n",
    "    \n",
    "dataset = CaptioningDataset(cands,refs,imgfeatfiles,data_type=args.dataset, use_idf=False)\n",
    "dataloader = DataLoader(dataset, 1, shuffle=False)\n",
    "layer = args.layer\n",
    "\n",
    "prs_a = []\n",
    "rcs_a = []\n",
    "f1s_a = []\n",
    "\n",
    "use_idf = False\n",
    "\n",
    "for text_a, input_mask_a, segment_ids_a, features, spatials, image_mask, co_attention_mask, input_idf_a, idxs_ in tqdm(iter(dataloader)):\n",
    "    text_a = text_a.cuda() \n",
    "    input_idf_a = input_idf_a.cuda()\n",
    "    input_mask_a = input_mask_a.cuda()\n",
    "    segment_ids_a = segment_ids_a.cuda()\n",
    "    features = features.cuda()\n",
    "    spatials = spatials.cuda()\n",
    "    image_mask = image_mask.cuda()\n",
    "    co_attention_mask = co_attention_mask.cuda()\n",
    "    task = [args.task]\n",
    "    task = torch.from_numpy(np.array(task)).cuda().unsqueeze(0).repeat(spatials.size(0), 1)\n",
    "    #break\n",
    "    with torch.no_grad():\n",
    "        p5a, r5a, f5a = compute_bert_score(model, text_a, input_mask_a, segment_ids_a, \n",
    "                                           features, spatials, image_mask, \n",
    "                                           co_attention_mask, input_idf_a, task, \n",
    "                                           0, 1, layer=layer)\n",
    "        if(len(prs_a) == 0):\n",
    "            prs_a = p5a\n",
    "            rcs_a = r5a\n",
    "            f1s_a = f5a            \n",
    "        else:\n",
    "            prs_a = np.concatenate((prs_a, p5a))\n",
    "            rcs_a = np.concatenate((rcs_a, r5a))\n",
    "            f1s_a = np.concatenate((f1s_a, f5a))\n",
    "\n",
    "scores_P = prs_a\n",
    "scores_R = rcs_a\n",
    "scores_F1 = f1s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_P = np.array(scores_P).reshape([-1,len(row['captSet_text'])])\n",
    "scores_R = np.array(scores_R).reshape([-1,len(row['captSet_text'])])\n",
    "scores_F1 = np.array(scores_F1).reshape([-1,len(row['captSet_text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ArtEmis , Number of caption sets: 15884\n",
      "ViLBERTscores_P\n",
      "Accuracy at errType=1:12264/15884= 0.772097708385797\n",
      "Accuracy at errType=2:8955/15884= 0.5637748677914883\n",
      "Accuracy at errType=3:2804/15884= 0.17652984134978594\n",
      "Accuracy for all types:24023/47652= 0.5041341391756904\n",
      "Dataset: ArtEmis , Number of caption sets: 15884\n",
      "ViLBERTscores_R\n",
      "Accuracy at errType=1:11217/15884= 0.7061823218332913\n",
      "Accuracy at errType=2:10864/15884= 0.6839587005791992\n",
      "Accuracy at errType=3:9130/15884= 0.574792243767313\n",
      "Accuracy for all types:31211/47652= 0.6549777553932679\n",
      "Dataset: ArtEmis , Number of caption sets: 15884\n",
      "ViLBERTscores_F1\n",
      "Accuracy at errType=1:12255/15884= 0.7715311004784688\n",
      "Accuracy at errType=2:9841/15884= 0.6195542684462352\n",
      "Accuracy at errType=3:4442/15884= 0.27965248048350544\n",
      "Accuracy for all types:26538/47652= 0.5569126164694032\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "no_errType = 3\n",
    "cnt_corr_all = 0\n",
    "cnt_incorr_all = 0\n",
    "print(\"Dataset:\",datasetname,\", Number of caption sets:\",len(scores_P))\n",
    "print(\"ViLBERTscores_P\")\n",
    "for errType in range(1,no_errType+1):\n",
    "    cnt_corr = 0\n",
    "    cnt_incorr = 0\n",
    "    for sim in scores_P:\n",
    "        if sim[0] > sim[errType]:\n",
    "            cnt_corr +=1\n",
    "            cnt_corr_all +=1\n",
    "        else:\n",
    "            cnt_incorr +=1\n",
    "            cnt_incorr_all +=1\n",
    "    print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))\n",
    "\n",
    "cnt_corr_all = 0\n",
    "cnt_incorr_all = 0\n",
    "print(\"Dataset:\",datasetname,\", Number of caption sets:\",len(scores_R))\n",
    "print(\"ViLBERTscores_R\")\n",
    "for errType in range(1,no_errType+1):\n",
    "    cnt_corr = 0\n",
    "    cnt_incorr = 0\n",
    "    for sim in scores_R:\n",
    "        if sim[0] > sim[errType]:\n",
    "            cnt_corr +=1\n",
    "            cnt_corr_all +=1\n",
    "        else:\n",
    "            cnt_incorr +=1\n",
    "            cnt_incorr_all +=1\n",
    "    print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))\n",
    "\n",
    "cnt_corr_all = 0\n",
    "cnt_incorr_all = 0\n",
    "print(\"Dataset:\",datasetname,\", Number of caption sets:\",len(scores_F1))\n",
    "print(\"ViLBERTscores_F1\")\n",
    "for errType in range(1,no_errType+1):\n",
    "    cnt_corr = 0\n",
    "    cnt_incorr = 0\n",
    "    for sim in scores_F1:\n",
    "        if sim[0] > sim[errType]:\n",
    "            cnt_corr +=1\n",
    "            cnt_corr_all +=1\n",
    "        else:\n",
    "            cnt_incorr +=1\n",
    "            cnt_incorr_all +=1\n",
    "    print(f\"Accuracy at errType={errType}:{cnt_corr}/{cnt_corr+cnt_incorr}=\",cnt_corr/(cnt_corr+cnt_incorr))\n",
    "\n",
    "print(f\"Accuracy for all types:{cnt_corr_all}/{cnt_corr_all+cnt_incorr_all}=\",cnt_corr_all/(cnt_corr_all+cnt_incorr_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
